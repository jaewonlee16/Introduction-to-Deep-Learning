{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AYONXr6kPGXv"
   },
   "source": [
    "# HW4-2 : Training GAN to generate pixel characters and items.\n",
    "This homework consists of 4 tasks:\n",
    "1. Implement Variational AutoEncoder (VAE) and its training\n",
    "2. ***Implement Generative Adversarial Network (GAN) and its training***\n",
    "3. ***Implement Conditonal Generative Adversarial Network (cGAN) and its training***\n",
    "4.  ***Implement Fréchet inception distance (FID) score***\n",
    "\n",
    "In this file, you are asked to perform ***Task 2,3,4*** and complete several functions and classes in `HW4_2_YourAnswer.py` and `HW4_fid_YourAnswer.py`.\n",
    "In the notebook, the place where students are required to complete the codes will be denoted as Exercise or TODO\n",
    "\n",
    "The score of this homework is mainly based on the \n",
    "- ***Whether generated image is plausible***(training is sucess or not) from your conditional GAN model which trained in Exercise 4\n",
    "- ***FID score implementation*** in Exercise 5\n",
    "- The ***generated image quality*** of your GAN model which trained in Exercise 6\n",
    "    - It is evaluated by FID score with TA's implementation. \n",
    "\n",
    "Be careful that the selection of hyperparameters and model architecture will affect the performance of your model. \n",
    "Change your code in `HW4_2_YourAnswer.py` and hyperparmeters in the `config` variable to achieve better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N984abTdPGXz"
   },
   "source": [
    "# Setting system path\n",
    "## TODO\n",
    "- Depending on your environment, select your proper code between `1 Setting on the colab` or `2. Setting on the local` Section. (You should comment out the other section)\n",
    "    - Although you train your model on the local, please train your model on the colab in `Exercise 6`. The evaluation with TA's FID score might be not executable on the local.\n",
    "- Set proper `path` variable to import `HW4_2_YourAnswer.py` and `HW4_fid_YourAnswer.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wi1wEOBCPGXz"
   },
   "source": [
    "### 1. Setting on the colab\n",
    "It allows to connect your google drive to colab and utilize files in your google drive \n",
    "\n",
    "Then, it changes the current directory to the path the howework folder is located.\n",
    "\n",
    "### TODO\n",
    "- Assign the path of your working directory (where the .ipynb file is located) to the variable named `path`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-12-01T17:16:49.234275Z",
     "iopub.status.busy": "2023-12-01T17:16:49.234075Z",
     "iopub.status.idle": "2023-12-01T17:16:49.237597Z",
     "shell.execute_reply": "2023-12-01T17:16:49.237046Z"
    },
    "executionInfo": {
     "elapsed": 3483,
     "status": "ok",
     "timestamp": 1701404594048,
     "user": {
      "displayName": "­배현웅 / 학생 / 협동과정 인공지능전공",
      "userId": "02355475606709969663"
     },
     "user_tz": -540
    },
    "id": "_aWgY7wB8hBq",
    "outputId": "06e2d514-06d8-4921-b99e-88fdbd39438f"
   },
   "outputs": [],
   "source": [
    "path = \"MyDrive/\"  ######## YOUR WORKING DIRECTORY PATH HERE ########\n",
    "def mount_drive():\n",
    "    from google.colab import drive\n",
    "    mount_location = '/content/drive'\n",
    "    drive.mount(mount_location,force_remount=True)\n",
    "    return mount_location\n",
    "import os, sys\n",
    "mount_location = mount_drive()\n",
    "print(\"your google drive is mounted at: \", mount_location)\n",
    "path = os.path.join(mount_location,path) # \"/content/drive/MyDrive/\"\n",
    "if os.path.exists(path):\n",
    "    print(\"Path exists\\n\", path)\n",
    "    sys.path.append(path)\n",
    "    print(\"Path added (file under this path automatically identified)\\n\", path)\n",
    "    os.chdir(path)\n",
    "else :\n",
    "    raise ValueError(\"Path does not exist. Set proper path \\n\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_tLA7M6XPGX1"
   },
   "source": [
    "### 2. Setting on the local\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T17:16:49.240087Z",
     "iopub.status.busy": "2023-12-01T17:16:49.239849Z",
     "iopub.status.idle": "2023-12-01T17:16:49.245858Z",
     "shell.execute_reply": "2023-12-01T17:16:49.245440Z"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1701404594048,
     "user": {
      "displayName": "­배현웅 / 학생 / 협동과정 인공지능전공",
      "userId": "02355475606709969663"
     },
     "user_tz": -540
    },
    "id": "C4KgLQ4bPGX1"
   },
   "outputs": [],
   "source": [
    "path='./'\n",
    "import os\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UrksjiCzPGX1"
   },
   "source": [
    "## Import package and set serveral configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-12-01T17:16:49.247859Z",
     "iopub.status.busy": "2023-12-01T17:16:49.247717Z",
     "iopub.status.idle": "2023-12-01T17:16:50.029047Z",
     "shell.execute_reply": "2023-12-01T17:16:50.028456Z"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1701404594048,
     "user": {
      "displayName": "­배현웅 / 학생 / 협동과정 인공지능전공",
      "userId": "02355475606709969663"
     },
     "user_tz": -540
    },
    "id": "THnoWwgJ9Ibi",
    "outputId": "a090aaae-dd59-4a43-c6e2-5d361b20f834"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lee/anaconda3/envs/torch_tutorial/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/lee/anaconda3/envs/torch_tutorial/lib/python3.11/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSs'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import sqrt\n",
    "from HW4_2_YourAnswer import *\n",
    "from HW4_fid_YourAnswer import calculate_fid\n",
    "from utils import *\n",
    "# It helps to reload the functions automatically\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "execution": {
     "iopub.execute_input": "2023-12-01T17:16:50.031927Z",
     "iopub.status.busy": "2023-12-01T17:16:50.031677Z",
     "iopub.status.idle": "2023-12-01T17:16:50.094184Z",
     "shell.execute_reply": "2023-12-01T17:16:50.093597Z"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1701404594048,
     "user": {
      "displayName": "­배현웅 / 학생 / 협동과정 인공지능전공",
      "userId": "02355475606709969663"
     },
     "user_tz": -540
    },
    "id": "DQ03So589YhR",
    "outputId": "117e6b1a-ae51-4562-9ea0-ec97a7abc69f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check device is 'cuda' if GPU is available\n",
    "device = 'cpu' if not torch.cuda.is_available() else 'cuda'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "- You should set the `config` variable to achieve better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T17:16:50.122717Z",
     "iopub.status.busy": "2023-12-01T17:16:50.122204Z",
     "iopub.status.idle": "2023-12-01T17:16:50.136350Z",
     "shell.execute_reply": "2023-12-01T17:16:50.135865Z"
    },
    "executionInfo": {
     "elapsed": 500,
     "status": "ok",
     "timestamp": 1701404594544,
     "user": {
      "displayName": "­배현웅 / 학생 / 협동과정 인공지능전공",
      "userId": "02355475606709969663"
     },
     "user_tz": -540
    },
    "id": "xtNqRI6p9mJJ"
   },
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "# The config is used to store various hyperparameters\n",
    "# Except for the ones mentioned below, you are free to add any hyperparameter you want\n",
    "# Due to evaluate the performance of your model,\n",
    "#   we will use the same diemension of the latent vector\n",
    "config = SimpleNamespace(\n",
    "    seed = 2023, # Do not change this\n",
    "    input_shape = (3,16,16), # Do not change this\n",
    "    num_classes = 5, # Do not change this\n",
    "    nc = 3, # Do not change this\n",
    "    test_batch_size = 64, # Do not change this\n",
    "    latent_dim =100, # Do not change this\n",
    "    ############################\n",
    "    # you need to set the values below\n",
    "\n",
    "    epoch = 20, # You can set under 20 epochs\n",
    "    batch_size = 5,\n",
    "    lr = 1e-5,\n",
    ")\n",
    "# usage : config.seed or config.batch_size, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "- Write your student number here as a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.student_number = '2019-12172' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T17:16:50.138378Z",
     "iopub.status.busy": "2023-12-01T17:16:50.138262Z",
     "iopub.status.idle": "2023-12-01T17:16:50.147248Z",
     "shell.execute_reply": "2023-12-01T17:16:50.146803Z"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1701404594544,
     "user": {
      "displayName": "­배현웅 / 학생 / 협동과정 인공지능전공",
      "userId": "02355475606709969663"
     },
     "user_tz": -540
    },
    "id": "1jdanZx89a9W"
   },
   "outputs": [],
   "source": [
    "# set randomness\n",
    "set_randomness(config.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vM3-hgUkPGX3"
   },
   "source": [
    "# Generative Adversarial Network\n",
    "\n",
    "GAN consists of two networks, ***Generator and Discriminator***, and you are asked to build these two networks.\n",
    "\n",
    "Afterward, you are asked to define ***dataloader, loss function, optimizer***,  ***Fréchet inception distance (FID) score*** which is evaluation metric for GAN and ***its training procedure***.\n",
    "\n",
    "To sum up, you need to implement the following things:\n",
    "- Your own Generator model and Discriminator model for GAN\n",
    "- Your own Generator model and Discriminator model for conditional GAN\n",
    "- Dataloader\n",
    "- Loss function\n",
    "- Optimizer\n",
    "- FID score\n",
    "- Training procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wn8Z11bEPGX3"
   },
   "source": [
    "# Exercise 1. Generator and discriminator model for GAN\n",
    "In this initial exercise, the task is to construct two deep neural networks, both comprising a Generative Adversarial Network (GAN). Furthermore, you are asked to build the networks of conditional GAN (cGAN) in Exercise 2.\n",
    "\n",
    "In this exercise, you are asked to implement the ***Generator*** and ***Discriminator*** model for GAN.\n",
    "\n",
    "## 1.1 Generator in Generative Adversarial Network\n",
    "First, let's implement generator. The generator for GAN is as below:\n",
    "- Generator : $G(z) = x$\n",
    "    - $z$ : Latent vector `(batch_size, latent_dim)`\n",
    "    - $x$ : Fake image `(batch_size, 1, 16, 16)`\n",
    "\n",
    "To do this, complete the implementation of the `Generator` class in `HW4_2_YourAnswer.py`.\n",
    "You need to fill two blocks in the `Generator` class.\n",
    "1. \\_\\_init\\_\\_ : Define model structure with `self.model`\n",
    "2. forward : Implement forward pass of `Generator`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ubKm1C7iPGX3"
   },
   "source": [
    "### 1.1.1 \\_\\_init\\_\\_\n",
    "In the class initialization, you need to build ***your own*** generator model which outputs 3x16x16 images. The arguments are explained in the `HW4_2_YourAnswer.py`. \n",
    "\n",
    "Recommandation : Set the number of model parameters is less than 1.5M (Then, training time is less than 10~12 minutes). \n",
    "\n",
    "See the [link](https://pytorch.org/docs/stable/nn.html ) to find proper layer in the pytorch package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gODdBAxjPGX4"
   },
   "source": [
    "### TODO\n",
    "- Build your own generator model in the \\_\\_init\\_\\_ (The code in `HW4_2_YourAnswer.py`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-12-01T17:16:50.149705Z",
     "iopub.status.busy": "2023-12-01T17:16:50.149509Z",
     "iopub.status.idle": "2023-12-01T17:16:50.839477Z",
     "shell.execute_reply": "2023-12-01T17:16:50.838859Z"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1701404594544,
     "user": {
      "displayName": "­배현웅 / 학생 / 협동과정 인공지능전공",
      "userId": "02355475606709969663"
     },
     "user_tz": -540
    },
    "id": "SgY0umNoPGX4",
    "outputId": "522cfcf4-674b-457b-ed37-b4413f0f8e97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=100, out_features=256, bias=True)\n",
      "    (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=512, out_features=1024, bias=True)\n",
      "    (7): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): Linear(in_features=1024, out_features=768, bias=True)\n",
      "    (10): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from HW4_2_YourAnswer import Generator\n",
    "gen = Generator(latent_dim = config.latent_dim,nc = config.nc).to(device)\n",
    "print(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-12-01T17:16:50.842135Z",
     "iopub.status.busy": "2023-12-01T17:16:50.841994Z",
     "iopub.status.idle": "2023-12-01T17:16:50.853038Z",
     "shell.execute_reply": "2023-12-01T17:16:50.852586Z"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1701404594544,
     "user": {
      "displayName": "­배현웅 / 학생 / 협동과정 인공지능전공",
      "userId": "02355475606709969663"
     },
     "user_tz": -540
    },
    "id": "CoIlEk3H64FY",
    "outputId": "663753bc-b21c-4453-d1c0-071d939e71b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of model parameters:  1473536\n"
     ]
    }
   ],
   "source": [
    "# check the number of parameters\n",
    "print(\"The number of model parameters: \", sum(p.numel() for p in gen.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nLxQRwHLPGX4"
   },
   "source": [
    "### 1.1.2 forward\n",
    "The forward function take input of the batch of latent vector and return output of the batch of (fake) image.\n",
    "### TODO\n",
    "- Implement `forward` function in `Generator` class (The code in `HW4_2_YourAnswer.py`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UteFA-dWPGX4"
   },
   "source": [
    "If implemented correctly, the output from the running the below cell is the `shape of image with batch (B,C,H,W)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-12-01T17:16:50.855684Z",
     "iopub.status.busy": "2023-12-01T17:16:50.855313Z",
     "iopub.status.idle": "2023-12-01T17:16:52.304533Z",
     "shell.execute_reply": "2023-12-01T17:16:52.303925Z"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1701404594544,
     "user": {
      "displayName": "­배현웅 / 학생 / 협동과정 인공지능전공",
      "userId": "02355475606709969663"
     },
     "user_tz": -540
    },
    "id": "JgzbbKPXPGX4",
    "outputId": "78a82d13-d6d2-45d3-c331-c32a8f6efd52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "z = torch.randn(config.batch_size, config.latent_dim).to(device)\n",
    "out = gen(z)\n",
    "print(out.shape)\n",
    "assert out.shape == (config.batch_size, config.nc, *config.input_shape[1:]), f\"Wrong output shape from the generator (expected {(config.batch_size, config.nc, *config.input_shape[1:])}, got {out.shape})\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C7lmuhOBPGX4"
   },
   "source": [
    "\n",
    "## 1.2 Discriminator in Generative Adversarial Network\n",
    "First, let's implement discriminator. To do this, complete the implementation of the `Discriminator` class in `HW4_2_YourAnswer.py`.\n",
    "\n",
    "Similarly, you need to fill two functions in the `Discriminator` class.\n",
    "1. \\_\\_init\\_\\_ : Define model structure with `self.model`\n",
    "2. forward : Implement forward pass of `Discriminator`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tCrBXmJiPGX4"
   },
   "source": [
    "### 1.2.1 \\_\\_init\\_\\_\n",
    "In the class initialization, you need to build ***your own*** discriminator model which returns the probability of being real. The arguments are explained in the `HW4_2_YourAnswer.py`.\n",
    "\n",
    "Recommandation : Set the number of model parameters is less than 1.5M (Then, training time is less than 10~12 minutes).\n",
    " \n",
    "\n",
    "See the [link](https://pytorch.org/docs/stable/nn.html ) to find proper layer in the pytorch package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3VHR6ypSPGX4"
   },
   "source": [
    "### TODO\n",
    "- Build the discriminator model in the \\_\\_init\\_\\_ (The code in `HW4_2_YourAnswer.py`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-12-01T17:16:52.307160Z",
     "iopub.status.busy": "2023-12-01T17:16:52.306976Z",
     "iopub.status.idle": "2023-12-01T17:16:52.326031Z",
     "shell.execute_reply": "2023-12-01T17:16:52.325497Z"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1701404594544,
     "user": {
      "displayName": "­배현웅 / 학생 / 협동과정 인공지능전공",
      "userId": "02355475606709969663"
     },
     "user_tz": -540
    },
    "id": "ddO30zLXPGX4",
    "outputId": "827c7ad2-c24d-41b1-8589-c12fabef1b09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator(\n",
      "  (model): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (8): Flatten(start_dim=1, end_dim=-1)\n",
      "    (9): Linear(in_features=4096, out_features=1, bias=True)\n",
      "    (10): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from HW4_2_YourAnswer import Discriminator\n",
    "dis = Discriminator(nc=config.nc).to(device)\n",
    "print(dis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-12-01T17:16:52.328396Z",
     "iopub.status.busy": "2023-12-01T17:16:52.328276Z",
     "iopub.status.idle": "2023-12-01T17:16:52.338802Z",
     "shell.execute_reply": "2023-12-01T17:16:52.338321Z"
    },
    "executionInfo": {
     "elapsed": 300,
     "status": "ok",
     "timestamp": 1701404594841,
     "user": {
      "displayName": "­배현웅 / 학생 / 협동과정 인공지능전공",
      "userId": "02355475606709969663"
     },
     "user_tz": -540
    },
    "id": "gUlt-6CA64FZ",
    "outputId": "e97cf54e-136c-4925-e431-4ab743b04b8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of model parameters:  660865\n"
     ]
    }
   ],
   "source": [
    "# check the number of parameters\n",
    "print(\"The number of model parameters: \",  sum(p.numel() for p in dis.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NOPiw7hyPGX8"
   },
   "source": [
    "### 1.2.2 forward\n",
    "The forward function take input of the batch of image and return output of the probability of being real for each batch.\n",
    "### TODO\n",
    "- Implement `forward` function in `Discriminator` class (The code in `HW4_2_YourAnswer.py`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g1II3IfqPGX8"
   },
   "source": [
    "If the function is correctly implemented, the output of the below cell would be `the probability of being real` with size of (B,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-12-01T17:16:52.341461Z",
     "iopub.status.busy": "2023-12-01T17:16:52.341128Z",
     "iopub.status.idle": "2023-12-01T17:16:52.353120Z",
     "shell.execute_reply": "2023-12-01T17:16:52.352625Z"
    },
    "executionInfo": {
     "elapsed": 274,
     "status": "ok",
     "timestamp": 1701404595112,
     "user": {
      "displayName": "­배현웅 / 학생 / 협동과정 인공지능전공",
      "userId": "02355475606709969663"
     },
     "user_tz": -540
    },
    "id": "NOcfXHm9t1ce",
    "outputId": "75ab4d8f-361d-4ba8-ef70-e2c12498c4e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1])\n"
     ]
    }
   ],
   "source": [
    "z = torch.randn(config.batch_size, config.latent_dim).to(device)\n",
    "out = gen(z)\n",
    "out1 = dis(out)\n",
    "print(out1.shape)\n",
    "assert (out1.shape == torch.Size([config.batch_size, 1])), f\"Discriminator output shape is wrong : {out1.shape}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2. Conditional GAN (cGAN)\n",
    "In this exercise, you are asked to implement conditional GAN (cGAN). The cGAN is similar to GAN, but it takes label as input with latent vector. The implementation sequence is as below:\n",
    "- Implement `cGenerator` for cGAN\n",
    "- Implement `cDiscriminator` for cGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Generator in cGAN\n",
    "Similar to GAN, you need to implement generator for cGAN. The generator for cGAN is as below:\n",
    "- Generator : $G(z,y) = x$\n",
    "    - $z$ : Latent vector `(batch_size, latent_dim)`\n",
    "    - $y$ : Label vector `(batch_size, num_classes)`\n",
    "    - $x$ : Fake image `(batch_size, 1, 16, 16)`\n",
    "\n",
    "To build your own model, complete the implementation of the `cGenerator` class in `HW4_2_YourAnswer.py`.\n",
    "You need to fill two blocks in the `cGenerator` class.\n",
    "1. \\_\\_init\\_\\_ : Define model structure with `self.model`\n",
    "2. forward : Implement forward pass of `cGenerator`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 \\_\\_init\\_\\_\n",
    "In the class initialization, you need to build the conditinal generator model with your own code. It takes input of latent vector and label and returns the fake image (size : 3x16x16). The arguments are explained in the `HW4_2_YourAnswer.py`.\n",
    "\n",
    "See the [link](https://pytorch.org/docs/stable/nn.html ) to find proper layer in the pytorch package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cGenerator(\n",
      "  (label_embedding): Embedding(5, 100)\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=105, out_features=256, bias=True)\n",
      "    (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=512, out_features=1024, bias=True)\n",
      "    (7): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): Linear(in_features=1024, out_features=768, bias=True)\n",
      "    (10): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from HW4_2_YourAnswer import *\n",
    "cond_gen = cGenerator(input_shape= config.input_shape,\n",
    "                 latent_dim=config.latent_dim, num_classes=config.num_classes, nc = config.nc\n",
    "                ).to(device)\n",
    "print(cond_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of model parameters:  1475316\n"
     ]
    }
   ],
   "source": [
    "# check the number of parameters\n",
    "print(\"The number of model parameters: \",  sum(p.numel() for p in cond_gen.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 forward\n",
    "The forward function take input of the batch of latent vector and return output of the batch of (fake) image.\n",
    "### TODO\n",
    "- Implement `forward` function in `cGenerator` class (The code in `HW4_2_YourAnswer.py`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If implemented correctly, the output from the running the below cell is the `shape of image with batch (B,C,H,W)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_embedding.shape=torch.Size([5, 500])\n",
      "input.shape=torch.Size([5, 100])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (5x600 and 105x256)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m z \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(config\u001b[38;5;241m.\u001b[39mbatch_size, config\u001b[38;5;241m.\u001b[39mlatent_dim)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      2\u001b[0m label \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(config\u001b[38;5;241m.\u001b[39mbatch_size, config\u001b[38;5;241m.\u001b[39mnum_classes)\u001b[38;5;241m.\u001b[39mscatter_(\u001b[38;5;241m1\u001b[39m, torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, config\u001b[38;5;241m.\u001b[39mnum_classes, (config\u001b[38;5;241m.\u001b[39mbatch_size,\u001b[38;5;241m1\u001b[39m)), \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 3\u001b[0m out \u001b[38;5;241m=\u001b[39m cond_gen(z, label)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(out\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m out\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m (config\u001b[38;5;241m.\u001b[39mbatch_size, config\u001b[38;5;241m.\u001b[39mnc, \u001b[38;5;241m*\u001b[39mconfig\u001b[38;5;241m.\u001b[39minput_shape[\u001b[38;5;241m1\u001b[39m:]), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrong output shape from the conditional generator\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_tutorial/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/intro_dl/hw4/HW4_2_YourAnswer.py:181\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(self, input, label)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m):\n\u001b[0;32m--> 181\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;124;03m    Initialize the dataloader class.\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \n\u001b[1;32m    184\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;124;03m        train : whether to use training dataset or test dataset (type : bool)\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;124;03m        batch_size : how many samples per batch to load (type : int)\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;28msuper\u001b[39m(dataloader, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size \u001b[38;5;241m=\u001b[39m batch_size\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_tutorial/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_tutorial/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_tutorial/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_tutorial/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (5x600 and 105x256)"
     ]
    }
   ],
   "source": [
    "z = torch.randn(config.batch_size, config.latent_dim).to(device)\n",
    "label = torch.zeros(config.batch_size, config.num_classes).scatter_(1, torch.randint(0, config.num_classes, (config.batch_size,1)), 1).to(device)\n",
    "out = cond_gen(z, label)\n",
    "print(out.shape)\n",
    "assert out.shape == (config.batch_size, config.nc, *config.input_shape[1:]), \"Wrong output shape from the conditional generator\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2.2 Discriminator in Conditional Generative Adversarial Network\n",
    "First, let's implement discriminator. To do this, complete the implementation of the `cDiscriminator` class in `HW4_2_YourAnswer.py`.\n",
    "Similarly, you need to fill two functions in the `cDiscriminator` class.\n",
    "1. \\_\\_init\\_\\_ : Define model structure with ` self.model`\n",
    "2. forward : Implement forward pass of `cDiscriminator`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 \\_\\_init\\_\\_\n",
    "Build your own model for `cDiscriminator`. The arguments are explained in the `HW4_2_YourAnswer.py`.\n",
    "\n",
    "See the [link](https://pytorch.org/docs/stable/nn.html ) to find proper layer in the pytorch package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "- Build the discriminator model in the cDiscriminator class (The code in `HW4_2_YourAnswer.py`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HW4_2_YourAnswer import cDiscriminator\n",
    "cond_dis = cDiscriminator(input_shape = config.input_shape,num_classes=config.num_classes).to(device)\n",
    "print(cond_dis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the number of parameters\n",
    "print(\"The number of model parameters: \", sum(p.numel() for p in cond_dis.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 forward\n",
    "The forward function take input of the batch of ***image and label***, then return output of the probability of being real for each batch. \n",
    "### TODO\n",
    "- Implement `forward` function in `cDiscriminator` class (The code in `HW4_2_YourAnswer.py`).\n",
    "\n",
    "If implemented correctly, the output from the running the below cell is the `the probability of being real with batch (B,1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.randn(config.batch_size, config.latent_dim).to(device)\n",
    "label = torch.zeros(config.batch_size, config.num_classes).scatter_(1, torch.randint(0, config.num_classes, (config.batch_size,1)), 1).to(device)\n",
    "out = cond_gen(z,label)\n",
    "out1 = cond_dis(out,label)\n",
    "print(out1.shape)\n",
    "assert (out1.shape == torch.Size([config.batch_size, 1])), f\"Discriminator output shape is wrong : {out1.shape}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aJg4DPWdPGX8"
   },
   "source": [
    "# Exercise 3 : Implement dataloader for training procedure for GAN\n",
    "To implement dataloader, you need to define `preprocessing` and `dataloader` for dataset. The implementation sequence is as below:\n",
    "1. implement `preprocessing` via `torchvision.transforms`\n",
    "    - The `preprocessing` consists some transformation or processsing before loading on the network, such as normalization, resize, or other data augmentation.\n",
    "2. Declare `dataset` with `torch.utils.data.Dataset`\n",
    "3. Create `dataloader` which directly feeds the image into GAN networks.\n",
    "    - It defines batch size, shuffling option, or etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2EwytxwkPGX8"
   },
   "source": [
    "### 3.1 Preprocessing the dataset\n",
    "In this exercise, we implement the transformation function via `torchvision.transforms`.\n",
    "***Define your own*** preprocessing function for the input via `torchvision.transforms`.\n",
    "    \n",
    "For more details, see the [link](https://pytorch.org/vision/0.9/transforms.html).\n",
    "\n",
    "### TODO\n",
    "- Implement transformation (preprocessing) for the input image in `dataloader` class (The code in `HW4_2_YourAnswer.py`).\n",
    "    - Please include transformation of `ToTensor` in the transformation since the model takes the input of torch tensor image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-12-01T17:16:52.355681Z",
     "iopub.status.busy": "2023-12-01T17:16:52.355564Z",
     "iopub.status.idle": "2023-12-01T17:16:52.480812Z",
     "shell.execute_reply": "2023-12-01T17:16:52.480175Z"
    },
    "executionInfo": {
     "elapsed": 878,
     "status": "ok",
     "timestamp": 1701404595987,
     "user": {
      "displayName": "­배현웅 / 학생 / 협동과정 인공지능전공",
      "userId": "02355475606709969663"
     },
     "user_tz": -540
    },
    "id": "Tx1man4YPGX8",
    "outputId": "7b60f1fa-e71d-405b-a8e7-6ce3a770f7b0"
   },
   "outputs": [],
   "source": [
    "from HW4_2_YourAnswer import *\n",
    "loader_for_test = dataloader(train=False, batch_size=config.batch_size)\n",
    "print(loader_for_test.transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t7Hsp7alPGX8"
   },
   "source": [
    "### 3.2 Implement dataset and dataloader\n",
    "In this practice, you are asked to implement dataset and dataloader.\n",
    "### TODO\n",
    "- Implement self.dataset to load sprite dataset using `torch.utils.data.TensorDataset`. (The code in `HW4_2_YourAnswer.py`)\n",
    "- Implement self.dataloader (The code in `HW4_2_YourAnswer.py`)\n",
    "\n",
    "If you implemented correctly, the output will be as below (Any transformation is allowed, but the shape of image should be `(3,16,16)` and the type of label should be `torch.Tensor`):\n",
    "\n",
    "```\n",
    "The shape of image from the dataloader : torch.Size([10, 3, 16, 16])\n",
    "The type of image from the dataloader : <class 'torch.Tensor'>\n",
    "The shape of label from the dataloader : torch.Size([10, 5])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-12-01T17:16:52.482909Z",
     "iopub.status.busy": "2023-12-01T17:16:52.482492Z",
     "iopub.status.idle": "2023-12-01T17:16:52.628697Z",
     "shell.execute_reply": "2023-12-01T17:16:52.628218Z"
    },
    "executionInfo": {
     "elapsed": 412,
     "status": "ok",
     "timestamp": 1701404596396,
     "user": {
      "displayName": "­배현웅 / 학생 / 협동과정 인공지능전공",
      "userId": "02355475606709969663"
     },
     "user_tz": -540
    },
    "id": "Efrpv4dYPGX8",
    "outputId": "0c03643d-7713-43fd-e2ec-7974711ce188"
   },
   "outputs": [],
   "source": [
    "from HW4_2_YourAnswer import *\n",
    "loader_for_test = dataloader(train=True, batch_size=10)\n",
    "for img, label in loader_for_test:\n",
    "    print(f\"The shape of image from the dataloader : {img.shape}\")\n",
    "    print(f\"The type of image from the dataloader : {type(img)}\")\n",
    "    print(f\"The shape of label from the dataloader : {label.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "imyfPg8NPGX9"
   },
   "source": [
    "If implemented correctly, the figure is presented with character or item images with label as below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "sample_img = cv2.imread(\"./test_file/sprites_img.png\")\n",
    "plt.imshow(sample_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "execution": {
     "iopub.execute_input": "2023-12-01T17:16:52.630169Z",
     "iopub.status.busy": "2023-12-01T17:16:52.630017Z",
     "iopub.status.idle": "2023-12-01T17:16:52.915222Z",
     "shell.execute_reply": "2023-12-01T17:16:52.914812Z"
    },
    "executionInfo": {
     "elapsed": 1238,
     "status": "ok",
     "timestamp": 1701404597632,
     "user": {
      "displayName": "­배현웅 / 학생 / 협동과정 인공지능전공",
      "userId": "02355475606709969663"
     },
     "user_tz": -540
    },
    "id": "rYXHVa6PPGX9",
    "outputId": "314b2231-eb27-4767-ff72-7f292cdeb780"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "img = img.numpy().transpose(0,2,3,1)\n",
    "label = torch.argmax(label,dim=1).numpy().squeeze()\n",
    "for i in range(10):\n",
    "  train_x = img[i]\n",
    "  train_y = label[i]\n",
    "\n",
    "  ax = fig.add_subplot(1, 10, i+1)\n",
    "  ax.imshow(train_x)\n",
    "  ax.set_title(str(train_y))\n",
    "  ax.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vC3BM5sjPGX9"
   },
   "source": [
    "## Exercise 4. Implement training procedure for GAN and cGAN\n",
    "In this exercise, you are asked to implement training procedure for GAN. To train, you need to implement training code, optimimzer and loss function for generator and discriminator. The implementation sequence is as below:\n",
    "- Implement `loss_function` for generator and discriminator\n",
    "- Implement `optimizer` for each network\n",
    "- Implement `training process` of GAN\n",
    "- Implement `training process` of cGAN\n",
    "- Implement `Fréchet inception distance (FID)` score\n",
    "\n",
    "The training process involves a back-and-forth competition between the generator and the discriminator, with each iteration refining both models until the generator produces high-quality, realistic data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZTHN8T30PGX9"
   },
   "source": [
    "### 4.1 Loss function\n",
    "To train GAN, you need to define loss function for generator and discriminator. \n",
    "The discriminator and the generator engage in a two-player minimax game with the value function $V(G,D)$ where each player has opposing objective.\n",
    "\n",
    "$min_G max_D V(D,G) = \\mathbb{E}_{x\\sim p_{data}(x)}[logD(x)] + \\mathbb{E}_{z\\sim p_z(z)}[log(1-D(G(z)))]$\n",
    "\n",
    "The loss function outputs the loss value for each network. It returns the loss value depending on the real image and the fake image.\n",
    "\n",
    "***Keep in mind***  that adding a small value to avoid an error when calculating the log."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B2EVgJOqPGX9"
   },
   "source": [
    "### TODO\n",
    "- Implement `loss_function` function for generator and discriminator (The code in `HW4_2_YourAnswer.py`). Depending on the implementation, the output of the loss function could be different, but the shape of the output should be `torch.Size([batch_size,1])`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "conKcyJIPGYB"
   },
   "source": [
    "### 4.2 Optimizer\n",
    "To train GAN, you need to define optimizer for generator and discriminator. The optimizer for generator and discriminator is as below:\n",
    "- Generator optimizer : Adam optimizer with learning rate `config.lr` and betas (0.5, 0.999)\n",
    "- Discriminator optimizer : Adam optimizer with learning rate `config.lr` and betas (0.5, 0.999)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3oPHPVkEPGYB"
   },
   "source": [
    "### TODO\n",
    "- Implement `optimizer` for generator and discriminator in the training_GAN class (The code in `HW4_2_YourAnswer.py`)\n",
    "    - Define your optimimizer which is in `torch.optim`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-12-01T17:16:52.930121Z",
     "iopub.status.busy": "2023-12-01T17:16:52.929979Z",
     "iopub.status.idle": "2023-12-01T17:16:52.939921Z",
     "shell.execute_reply": "2023-12-01T17:16:52.939484Z"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1701404597632,
     "user": {
      "displayName": "­배현웅 / 학생 / 협동과정 인공지능전공",
      "userId": "02355475606709969663"
     },
     "user_tz": -540
    },
    "id": "ICGFmj9sPGYC",
    "outputId": "2b822d27-b968-4e4c-9342-8cda7456ce41"
   },
   "outputs": [],
   "source": [
    "trainer_test = training_GAN(train_loader = None, \\\n",
    "                                generator = gen, discriminator = dis, \\\n",
    "                                device=device, config = config)\n",
    "print(\"===== Optimizer for the generator =====\")\n",
    "print(trainer_test.optimizer_G)\n",
    "print(\"===== Optimizer for the discriminator =====\")\n",
    "print(trainer_test.optimizer_D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UmEBd-RcPGYC"
   },
   "source": [
    "### 4.3 Training process for GAN\n",
    "To train GAN, you need to implement training process for generator and discriminator. The things you should implemented for generator and discriminator are as below:\n",
    "- Discriminator training :\n",
    "    - Generate random latent vector\n",
    "    - Feed the latent vector into generator and get fake image `with no gradient`\n",
    "    - Feed the fake image into discriminator and get discriminator output\n",
    "    - Feed the real image into discriminator and get discriminator output\n",
    "    - Calculate discriminator loss with two discriminator outputs\n",
    "    - Update discriminator with discriminator loss\n",
    "- Generator training :\n",
    "    - Generate random latent vector\n",
    "    - Feed the latent vector into generator and get fake image\n",
    "    - Feed the fake image into discriminator and get discriminator output\n",
    "    - Calculate generator loss with discriminator output\n",
    "    - Update generator with generator loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Pv5b6LSPGYC"
   },
   "source": [
    "### TODO\n",
    "- Implement `one_iter_train` for generator and discriminator in the training_GAN class (The code in `HW4_2_YourAnswer.py`)\n",
    "    - Use `self.generator` and `self.discriminator` to access generator and discriminator\n",
    "    - Use `loss_function` to calculate loss\n",
    "    - Use `self.optimizer_G` and `self.optimizer_D` to update generator and discriminator\n",
    "\n",
    "The provided code below serves as a test for the training process based on the preceding instructions. After reviewing the output of the cell, modifications is allowed if you think it is necessary to enhance the training.\n",
    "You don't need to exactly follow the instructions   If `one_iter_train` is implemented correctly, the output of the below cell will be as below:\n",
    "```\n",
    "your implementation outcome is\n",
    " {'loss_G': 6.181875705718994, 'loss_D': 0.6788209676742554}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 410
    },
    "execution": {
     "iopub.execute_input": "2023-12-01T17:16:52.951516Z",
     "iopub.status.busy": "2023-12-01T17:16:52.951383Z",
     "iopub.status.idle": "2023-12-01T17:16:53.098657Z",
     "shell.execute_reply": "2023-12-01T17:16:53.098079Z"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "error",
     "timestamp": 1701404597632,
     "user": {
      "displayName": "­배현웅 / 학생 / 협동과정 인공지능전공",
      "userId": "02355475606709969663"
     },
     "user_tz": -540
    },
    "id": "fv9rwuV6PGYC",
    "outputId": "f19e211f-c95c-4ad4-e07b-d8445e205d40"
   },
   "outputs": [],
   "source": [
    "from HW4_2_YourAnswer import *\n",
    "from utils import test_one_iter_train_GAN\n",
    "from copy import deepcopy\n",
    "test_config = deepcopy(config)\n",
    "test_config.batch_size =2\n",
    "test_config.lr = 2e-4\n",
    "\n",
    "results = test_one_iter_train_GAN(training_GAN,test_config,device)\n",
    "\n",
    "print(\"your implementation outcome is\\n\",results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let' train GAN with your implementation\n",
    "Let's start training GAN. However, it is not guaranteed to train well since it is very sensitive to the hyperparameter.\n",
    "\n",
    "In this practice, you are asked to train GAN with your implementation.\n",
    "***Please check your generator generates the plausible image.*** If not, try to change the hyperparameter or model architecture. \n",
    "\n",
    "***Don't waste your time in training better generator in this exercise***. Just check that it is trained well and move on to the next exercise. The later exercise (Exercise 6) will ask you to train your model with your implementation. Then, evaluate the performance of your model.\n",
    "\n",
    "`train` function in `training_GAN` class will train the GAN model with your implementation. The `train` function will return the trained generator and discriminator model even though the interrupt occurs during the training process.\n",
    "\n",
    "The epoch in the `test_config` is set to 10, but you can train more than 10 epochs if needed (but, 10 epochs might be enough to check whether the generator is well trained).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HW4_2_YourAnswer import *\n",
    "from copy import deepcopy\n",
    "test_config = deepcopy(config)\n",
    "test_config.epoch = 10\n",
    "\n",
    "train_loader = dataloader(train=True, batch_size=test_config.batch_size)\n",
    "\n",
    "generator = Generator(latent_dim = test_config.latent_dim,nc = test_config.nc).to(device)\n",
    "discriminator = Discriminator(nc=test_config.nc).to(device)\n",
    "\n",
    "\n",
    "\n",
    "trainer_test = training_GAN(train_loader, \\\n",
    "                                generator = generator, discriminator = discriminator, \\\n",
    "                                device=device, config = test_config, fid_score_on=False,\n",
    "                                save_model=False,img_show=True,evaluation_on=True)\n",
    "results = trainer_test.train()\n",
    "trained_generator = results['generator']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of the training result\n",
    "If you implement the `training_GAN` class correctly, you can get the following result:\n",
    "- The loss curve of the GAN\n",
    "- Visualization of the output image created by the generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the trained model and loss history\n",
    "trained_generator = results['generator']\n",
    "trained_discriminator = results['discriminator']\n",
    "loss_G = results['G_loss_history']\n",
    "loss_D = results['D_loss_history']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss curve of GAN\n",
    "With the following code, you can visualize the loss curve of the GAN. The generator and discriminator loss are plotted in the same figure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_G, label='loss_G')\n",
    "plt.plot(loss_D, label='loss_D')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of the output image\n",
    "If you implement the `training_GAN` class correctly, you can see the visualization of the output image like below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import visualizeGAN\n",
    "visualizeGAN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of the generated image\n",
    "- The below output is the result of the GAN training with your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fake image generation\n",
    "show_image_with_GAN(trained_generator,config,device=device,cols=10,rows=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Training process for conditional GAN\n",
    "To train conditional GAN, you need to implement training process for conditional generator and discriminator. The things you should implemented for generator and discriminator are as below:\n",
    "- Discriminator training :\n",
    "    - Generate random latent vector\n",
    "    - Feed the latent vector and ***label*** into generator and get fake image `with no gradient`\n",
    "    - Feed the fake image into discriminator and get discriminator output\n",
    "    - Feed the real image into discriminator and get discriminator output\n",
    "    - Calculate discriminator loss with two discriminator outputs\n",
    "    - Update discriminator with discriminator loss\n",
    "- Generator training :\n",
    "    - Generate random latent vector\n",
    "    - Feed the latent vector and ***label*** into generator and get fake image\n",
    "    - Feed the fake image into discriminator and get discriminator output\n",
    "    - Calculate generator loss with discriminator output\n",
    "    - Update generator with generator loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HW4_2_YourAnswer import *\n",
    "from copy import deepcopy\n",
    "test_config = deepcopy(config)\n",
    "test_config.batch_size =2\n",
    "images = torch.load('./test_file/test_images.pth',map_location=device)\n",
    "labels = torch.load('./test_file/test_labels.pth',map_location=device)\n",
    "\n",
    "generator = cGenerator(input_shape= config.input_shape,\n",
    "                 latent_dim=config.latent_dim,num_classes=config.num_classes, nc = config.nc\n",
    "                ).to(device)\n",
    "discriminator = cDiscriminator(input_shape = config.input_shape,num_classes=config.num_classes).to(device)\n",
    "\n",
    "trainer = training_cGAN(train_loader = None, \\\n",
    "                            generator = generator, discriminator = discriminator, \\\n",
    "                            device=device, config = test_config)\n",
    "\n",
    "results = trainer.one_iter_train(images,labels)\n",
    "\n",
    "print(\"your implementation outcome is\\n\",results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let' train conditional GAN with your implementation\n",
    "Let's start training conditional GAN like GAN. However, it is not guaranteed to train well since it is very sensitive to the hyperparameter.\n",
    "\n",
    "In this practice, try to train conditional GAN with your implementation.\n",
    "***Please check your generator generates the plausible image.*** If not, try to change the hyperparameter or model architecture. \n",
    "\n",
    "Unlike training GAN which measures the quality of the generated image with FID score evaluted in Exercise 6, training conditional GAN will be evaluated whether the generated image is plausible or not (success or not). \n",
    "\n",
    "The epoch in the `test_config` is set to 10, but you can train more than 10 epochs and less than 20 epochs if needed (but, 10 epochs might be enough to check whether the generator is well trained).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HW4_2_YourAnswer import *\n",
    "import copy\n",
    "test_config = copy.deepcopy(config)\n",
    "train_loader = dataloader(train=True, batch_size=test_config.batch_size)\n",
    "\n",
    "generator =  cGenerator(input_shape= test_config.input_shape,\n",
    "                 latent_dim=test_config.latent_dim,num_classes=test_config.num_classes, nc = test_config.nc\n",
    "                ).to(device)\n",
    "discriminator = cDiscriminator(input_shape = test_config.input_shape,\\\n",
    "                               num_classes=test_config.num_classes).to(device)\n",
    "\n",
    "trainer_test = training_cGAN(train_loader, \\\n",
    "                                generator = generator, discriminator = discriminator, \\\n",
    "                                device=device, config = test_config, fid_score_on=False,\n",
    "                                save_model=True,img_show=True,evaluation_on=True)\n",
    "results = trainer_test.train()\n",
    "trained_generator = results['generator']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the trained model and loss history\n",
    "trained_generator = results['generator']\n",
    "trained_discriminator = results['discriminator']\n",
    "loss_G = results['G_loss_history']\n",
    "loss_D = results['D_loss_history']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss curve of condtional GAN\n",
    "With the following code, you can visualize the loss curve of the condtional GAN. The generator and discriminator loss are plotted in the same figure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_G, label='loss_G')\n",
    "plt.plot(loss_D, label='loss_D')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of the generated image\n",
    "- The below output is the result of the condtional GAN training with your implementation. The output image is generated with the random latent vector and label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fake image generation with your generator \n",
    "# it produces image depending on the label\n",
    "show_image_with_label(trained_generator,test_config,device=device,cols=6,rows=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5. Fréchet inception distance (FID) score\n",
    "As you see, the training process of GAN is not stable and loss is not a good indicator for the performance of GAN. To evaluate the performance of GAN, the Fréchet inception distance is suggested. You need to implement it here. The FID score is as below:\n",
    "- FID score : $FID = ||\\mu_{r} - \\mu_{g}||^{2}_{2} + Tr(\\Sigma_{r} + \\Sigma_{g} - 2(\\Sigma_{r}\\Sigma_{g})^{1/2})$\n",
    "    - $\\mu_{r}$ : Mean of real image's feature `(2048,)`\n",
    "    - $\\mu_{g}$ : Mean of fake image's feature `(2048,)`\n",
    "    - $\\Sigma_{r}$ : Covariance of real image's feature `(batch_size, batch_size)`\n",
    "    - $\\Sigma_{g}$ : Covariance of fake image's feature `(batch_size, batch_size)`\n",
    "    - All features are extracted from the pretrained  Inception-v3 network. In our code, which is defined in `PartialInceptionNetwork` class. `(batch_size, 2048)`\n",
    "    \n",
    "In this exercise, you are asked to implement FID score. The `calculate_fid_score` function is already implemented, but the function called in the function is not implemented. You need to implement the following functions to work `calculate_fid_score` function:\n",
    "- `get_statistics` : Calculate mean and covariance of the extracted feature\n",
    "- `get_activation` : It is called in the `get_statistics` function. It extracts feature from the pretrained Inception-v3 network\n",
    "- `calculate_frechet_distance` : Calculate FID score with mean and covariance of the extracted feature\n",
    "\n",
    "\n",
    "How `calculate_fid` function works\n",
    "- preprocessing the images with `preprocess_images` function (already implemented)\n",
    "- The `get_activation` function extract features from the `real_images` and `fake_images` via `get_activations` function\n",
    "- The `get_statistics` function calculate mean and covariance of features extracted from `get_activations` function\n",
    "- The `calculate_frechet_distance` function calculate FID score with the mean and covariance of features\n",
    "- The `calculate_fid` function return FID score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zFxWblGLPGYD"
   },
   "source": [
    "### TODO\n",
    "- Implement `get_activations` function to extract features from the pretrained Inception-v3 network (`inception_network` in the code) (The code in `HW4_fid_YourAnswer.py`).\n",
    "- Implement `get_statistics` to calculate mean and covariance of features (The code in `HW4_fid_YourAnswer.py`).\n",
    "- Implement `calculate_frechet_distance` and  to calculate FID score (The code in `HW4_fid_YourAnswer.py`).\n",
    "\n",
    "Each function checks output shape (except calculate_frechet_distance) whether the function is well implemented.\n",
    "\n",
    "Below the cell, we are going to check `calculate_frechet_distance` function\n",
    "\n",
    "If implemented correctly, the fid_1 is much larger than fid_2 (The fid_1 is about 300 ~ 400 and fid_2 is about 20~70). The batch size passed to the `calculate_fid_score` cannot be larget than the batch size of image passed to the function (In this case, the batch size of image is 1).\n",
    "\n",
    "Another way to check the implementation is set `fid_score_on=True` in `training_GAN` or `training_cGAN` class. If the implementation is correct, the fid score will be printed during the training process. Furthermore, the fid score becomes lower as the training process proceeds if you find the proper hyperparameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T17:18:04.296916Z",
     "iopub.status.busy": "2023-12-01T17:18:04.296754Z",
     "iopub.status.idle": "2023-12-01T17:18:04.544115Z",
     "shell.execute_reply": "2023-12-01T17:18:04.543566Z"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "aborted",
     "timestamp": 1701404597632,
     "user": {
      "displayName": "­배현웅 / 학생 / 협동과정 인공지능전공",
      "userId": "02355475606709969663"
     },
     "user_tz": -540
    },
    "id": "M5oQhts064Fh"
   },
   "outputs": [],
   "source": [
    "from utils import get_MNIST_image_and_visualize\n",
    "image1, image2, image3 = get_MNIST_image_and_visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T17:18:04.546751Z",
     "iopub.status.busy": "2023-12-01T17:18:04.546542Z",
     "iopub.status.idle": "2023-12-01T17:18:04.899525Z",
     "shell.execute_reply": "2023-12-01T17:18:04.899107Z"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "aborted",
     "timestamp": 1701404597633,
     "user": {
      "displayName": "­배현웅 / 학생 / 협동과정 인공지능전공",
      "userId": "02355475606709969663"
     },
     "user_tz": -540
    },
    "id": "k67-wLYoPGYD"
   },
   "outputs": [],
   "source": [
    "# Calculate FID score\n",
    "from HW4_fid_YourAnswer import calculate_fid, PartialInceptionNetwork\n",
    "import cv2\n",
    "inception_net = PartialInceptionNetwork()\n",
    "fid_1 = calculate_fid(inception_net,image1, image2,batch_size=1,device=device)\n",
    "fid_2 = calculate_fid(inception_net,image2, image3,batch_size=1,device=device)\n",
    "print(f\"fid_1 : {fid_1:.4f} >> fid_2 : {fid_2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e9D5ZFbOPGYC"
   },
   "source": [
    "## Exercise 6. Let'train and try to obtain good performance of GAN\n",
    "Let's start training with implemented training process. However, it might fails to converge since it is very sensitive to the hyperparameter. Therefore, try to tune the hyperparameter in the `config` class and model architecture to achieve better performance.\n",
    "### Training_GAN parameter options\n",
    "- `fid_score_on`\n",
    "    - If you does not implement `FID` score yet, set argument `fid_score_on=False` in the `training_GAN` function. However, it is recommended to implement `FID` score to evaluate the performance of GAN.\n",
    "- `img_show`\n",
    "    - If you want to see the generated for each epoch, set `img_show=True`.  The `img_show` argument is to show the generated image during training. \n",
    "\n",
    "### You will be evaluated with the FID score which calculated in the last epoch. The evaluation results will be as below:\n",
    "```\n",
    "EPOCH {config.epochs} - FID score : xxx.xxxx (evaluation)\n",
    "```\n",
    "After training, check the `GAN_generated_img.gif` in your `path`. You can see the generated image as epoch changes.\n",
    "When submitting the homework, please submit the `GAN_model.pth` file which generated in your `path`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T17:16:53.100839Z",
     "iopub.status.busy": "2023-12-01T17:16:53.100408Z",
     "iopub.status.idle": "2023-12-01T17:18:02.633075Z",
     "shell.execute_reply": "2023-12-01T17:18:02.632482Z"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "aborted",
     "timestamp": 1701404597632,
     "user": {
      "displayName": "­배현웅 / 학생 / 협동과정 인공지능전공",
      "userId": "02355475606709969663"
     },
     "user_tz": -540
    },
    "id": "_mOsl4yzPGYC"
   },
   "outputs": [],
   "source": [
    "from HW4_2_YourAnswer import *\n",
    "\n",
    "train_loader = dataloader(train=True, batch_size=config.batch_size)\n",
    "\n",
    "generator = Generator(latent_dim = config.latent_dim, nc = config.nc).to(device)\n",
    "discriminator = Discriminator(nc=config.nc).to(device)\n",
    "\n",
    "\n",
    "\n",
    "trainer_test = training_GAN(train_loader, \\\n",
    "                                generator = generator, discriminator = discriminator, \\\n",
    "                                device=device, config = config, fid_score_on=False,\n",
    "                                save_model=True,img_show=True,evaluation_on=True)\n",
    "results = trainer_test.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T17:18:02.635060Z",
     "iopub.status.busy": "2023-12-01T17:18:02.634936Z",
     "iopub.status.idle": "2023-12-01T17:18:02.644890Z",
     "shell.execute_reply": "2023-12-01T17:18:02.644545Z"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "aborted",
     "timestamp": 1701404597632,
     "user": {
      "displayName": "­배현웅 / 학생 / 협동과정 인공지능전공",
      "userId": "02355475606709969663"
     },
     "user_tz": -540
    },
    "id": "Gh3OzDILPGYC"
   },
   "outputs": [],
   "source": [
    "# load the trained model and loss history\n",
    "trained_generator = results['generator']\n",
    "trained_discriminator = results['discriminator']\n",
    "loss_G = results['G_loss_history']\n",
    "loss_D = results['D_loss_history']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss curve of GAN\n",
    "With the following code, you can visualize the loss curve of the GAN. The generator and discriminator loss are plotted in the same figure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_G, label='loss_G')\n",
    "plt.plot(loss_D, label='loss_D')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of the generated image\n",
    "- The below output is the result of the GAN training with your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fake image generation\n",
    "show_image_with_GAN(trained_generator,config,device=device,cols=10,rows=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gpzj6nGuPGYF"
   },
   "source": [
    "## Acknowledgments\n",
    "- Dataset : Sprites by ElvGames, [FrootsnVeggies](https://zrghr.itch.io/froots-and-veggies-culinary-pixels) and [kyrise](https://kyrise.itch.io/)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
