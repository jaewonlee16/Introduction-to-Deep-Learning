{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4MWH5UqdRLDA"
   },
   "source": [
    "# HW4-1 : Training generative model to generate MNIST digits\n",
    "This homework consists of 4 tasks:\n",
    "1. ***Implement Variational AutoEncoder (VAE) and its training***\n",
    "2. Implement Generative Adversarial Network (GAN) and its training\n",
    "3. Implement Conditonal Generative Adversarial Network (cGAN) and its training\n",
    "4. Implement Fréchet inception distance (FID) score\n",
    " \n",
    "\n",
    "In this file, you are asked to perform ***Task 1*** and complete several functions and classes in `HW4_1_YourAnswer.py`. In the notebook, the place where students are required to complete the codes will be denoted as Exercise or TODO\n",
    "\n",
    "The score for this homework is based on:\n",
    "- How correctly you implement the code (Each component will be tested)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CUFzXvWzRLDC"
   },
   "source": [
    "# Setting system path\n",
    "## TODO\n",
    "- Depending on your environment, select your proper code between `1 Setting on the colab` or `2. Setting on the local` Section. (You should comment out the other section)\n",
    "- Set proper `path` variable to import `HW4_1_YourAnswer.py` and `your_answer_fid.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wyIR2OMcRLDC"
   },
   "source": [
    "### 1. Setting on the colab\n",
    "It allows to connect your google drive to colab and utilize files in your google drive \n",
    "\n",
    "Then, it changes the current directory to the path the howework folder is located.\n",
    "\n",
    "### TODO\n",
    "- Assign the path of your working directory (where the .ipynb file is located) to the variable named `path`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-12-01T17:20:05.544483Z",
     "iopub.status.busy": "2023-12-01T17:20:05.544279Z",
     "iopub.status.idle": "2023-12-01T17:20:05.547673Z",
     "shell.execute_reply": "2023-12-01T17:20:05.547122Z"
    },
    "executionInfo": {
     "elapsed": 4508,
     "status": "ok",
     "timestamp": 1701530542368,
     "user": {
      "displayName": "배현웅",
      "userId": "14643794117352663059"
     },
     "user_tz": -540
    },
    "id": "d9ELU3Pv_0Pj",
    "outputId": "ec5a6a04-88b0-48ac-9e84-89a4060a2785"
   },
   "outputs": [],
   "source": [
    "# path = \"MyDrive/\"  ######## YOUR WORKING DIRECTORY PATH HERE ########\n",
    "# def mount_drive():\n",
    "#     from google.colab import drive\n",
    "#     mount_location = '/content/drive'\n",
    "#     drive.mount(mount_location,force_remount=True)\n",
    "\n",
    "#     return mount_location\n",
    "# import os, sys\n",
    "# mount_location = mount_drive()\n",
    "# print(\"your google drive is mounted at: \", mount_location)\n",
    "# path = os.path.join(mount_location,path) # \"/content/drive/MyDrive/\"\n",
    "# if os.path.exists(path):\n",
    "#     print(\"Path exists\\n\\t\", path)\n",
    "#     sys.path.append(path)\n",
    "#     print(\"Path added (file under this path automatically identified)\\n\\t\", path)\n",
    "#     os.chdir(path)\n",
    "# else :\n",
    "#     raise ValueError(\"Path does not exist. Set proper path \\n\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p9HbcNa3RLDD"
   },
   "source": [
    "### 2. Setting on the local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T17:20:05.550430Z",
     "iopub.status.busy": "2023-12-01T17:20:05.550082Z",
     "iopub.status.idle": "2023-12-01T17:20:05.556310Z",
     "shell.execute_reply": "2023-12-01T17:20:05.555859Z"
    },
    "id": "_kfW4OJWRLDD"
   },
   "outputs": [],
   "source": [
    "path='./'\n",
    "import os\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check whether the path is set correctly in the homework folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023_SNU_DL_HW4_1.ipynb  fid_score.cpython-310-x86_64-linux-gnu.so\r\n",
      "2023_SNU_DL_HW4_2.ipynb  sprite_labels_nc_1788_16x16.npy\r\n",
      "HW4_1_YourAnswer.py\t sprites_1788_16x16.npy\r\n",
      "HW4_2_YourAnswer.py\t test_GAN_one_iter.cpython-310-x86_64-linux-gnu.so\r\n",
      "HW4_fid_YourAnswer.py\t test_file\r\n",
      "__pycache__\t\t utils.py\r\n"
     ]
    }
   ],
   "source": [
    "!ls $path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CYT5M8RHRLDD"
   },
   "source": [
    "## Import package and set serveral configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-12-01T17:20:05.558086Z",
     "iopub.status.busy": "2023-12-01T17:20:05.557855Z",
     "iopub.status.idle": "2023-12-01T17:20:06.322438Z",
     "shell.execute_reply": "2023-12-01T17:20:06.321900Z"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1701530542369,
     "user": {
      "displayName": "배현웅",
      "userId": "14643794117352663059"
     },
     "user_tz": -540
    },
    "id": "pOLo0XEtxydQ",
    "outputId": "73ff33fa-4daa-49ef-bccf-e4e03d127646"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from math import sqrt\n",
    "from HW4_1_YourAnswer import *\n",
    "from utils import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you need to change hyperparameter in `config`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T17:20:06.324912Z",
     "iopub.status.busy": "2023-12-01T17:20:06.324645Z",
     "iopub.status.idle": "2023-12-01T17:20:06.333767Z",
     "shell.execute_reply": "2023-12-01T17:20:06.333328Z"
    },
    "id": "NA9OlQk_xydR"
   },
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "\n",
    "# The config is used to store various hyperparameters\n",
    "# you don't need to change it\n",
    "# you can access the hyperparameters by config.xxx\n",
    "config = SimpleNamespace(\n",
    "    seed = 1,\n",
    "    batch_size = 256,\n",
    "    test_batch_size = 100,\n",
    "    lr = 2e-4,\n",
    "    epoch = 10,\n",
    "    latent_dim = 2,\n",
    "    input_shape = (1,16,16),\n",
    "    hidden_dims = [256,128,64,32],\n",
    "    expand_dim = 2,\n",
    "    beta = 2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "- Write your student number here as a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.student_number = '2019-12172' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "execution": {
     "iopub.execute_input": "2023-12-01T17:20:06.336091Z",
     "iopub.status.busy": "2023-12-01T17:20:06.335776Z",
     "iopub.status.idle": "2023-12-01T17:20:06.395114Z",
     "shell.execute_reply": "2023-12-01T17:20:06.394640Z"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1701530542369,
     "user": {
      "displayName": "배현웅",
      "userId": "14643794117352663059"
     },
     "user_tz": -540
    },
    "id": "bFA_tM8gRLDE",
    "outputId": "791fe312-5f91-45a5-870d-34c9b5a79c5f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check device is 'cuda' if GPU is available\n",
    "device = 'cpu' if not torch.cuda.is_available() else 'cuda'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T17:20:06.424034Z",
     "iopub.status.busy": "2023-12-01T17:20:06.423601Z",
     "iopub.status.idle": "2023-12-01T17:20:06.438275Z",
     "shell.execute_reply": "2023-12-01T17:20:06.437815Z"
    },
    "id": "lKTSX-GkxydR"
   },
   "outputs": [],
   "source": [
    "# set randomness\n",
    "set_randomness(config.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "phiSy2WrRLDF"
   },
   "source": [
    "# Beta Variational AutoEncoder (beta-VAE)\n",
    "The exercise in this section is based on the following paper:\n",
    "- [Auto-Encoding Variational Bayes](https://arxiv.org/abs/1312.6114)\n",
    "- [beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework](https://openreview.net/pdf?id=Sy2fzU9gl)\n",
    "\n",
    "In this section, you are asked to implement beta Variational AutoEncoder (beta-VAE) and training. The VAE is a generative model that can be trained in an unsupervised manner. The beta-VAE is a variant of the VAE that can learn disentangled representations. They introduce an adjustable hyperparameter `beta` that balances latent channel capacity and independence constraints with reconstruction accuracy.\n",
    "\n",
    "The only difference between VAE and beta-VAE is the `loss function`. The beta-VAE and VAE have no difference in terms of the architecture. Therefore, we `take the same approach as the VAE` except for constructing the loss function.\n",
    "\n",
    "The VAE is composed of two parts: an encoder and a decoder. The encoder encodes the input data into a latent vector, and the decoder decodes the latent vector into the output data. The encoder and decoder are trained simultaneously. \n",
    "\n",
    "Before you implement the VAE, you need to build your own AutoEncoder(AE). The encoder in the AE encodes the input data into a latent vector, and it is directly passed to the decoder. The decoder in the AE decodes the latent vector into the output data. The encoder and decoder are trained simultaneously. \n",
    "\n",
    "The difference between AE and VAE is that the AE does not consider the distribution of the latent vector. The AE is trained to minimize the reconstruction error between the input data and the output data. On the other hand, the VAE is trained to minimize the reconstruction error and the KL divergence between the latent vector and the prior distribution. (The identical reconstruction error will be used) For later comparison between AutoEncoder and Variational AutoEncoder, you are asked to implement shared model architecture for both AutoEncoder and Variational AutoEncoder.\n",
    "\n",
    "Here is how the input data flows and the output are generated for the AutoEncoder and Variational AutoEncoder (self.xxx indicates the layer or model in the `Encoder` class):\n",
    "```\n",
    "    AutoEncoder \n",
    "        - input -> self.model -> self.fc_mean -> final output (will be passed to the decoder)\n",
    "    \n",
    "    Variational AutoEncoder\n",
    "        - input -> self.model -> intermediate output\n",
    "        - intermediate output -> self.fc_mean    -> output (one latent vector, mean)\n",
    "        - intermediate output -> self.fc_logvar  -> output (one latent vector, logvar)\n",
    "        - two latent vectors (mean and logvar)   -> self.reparameterize -> final output (will be passed to the decoder)\n",
    "```\n",
    "\n",
    "Here is the config for the AutoEncoder and Variational AutoEncoder:\n",
    "```\n",
    "    AutoEncoder\n",
    "        - beta: 0\n",
    "        - loss function: reconstruction loss\n",
    "    \n",
    "    Variational AutoEncoder\n",
    "        - beta: 1\n",
    "        - loss function: VAE loss (reconstruction loss + KL divergence)\n",
    "```\n",
    "The only difference between AE and VAE is the `loss function` and `reparameterization`.\n",
    "\n",
    "\n",
    "You are asked to implement the following functions and classes for AutoEncoder and Variational AutoEncoder in `HW4_1_YourAnswer.py`:\n",
    "- `Encoder` class for both AE and VAE\n",
    "- `Decoder` class\n",
    "- `loss function` concerning Evidence Lower Bound (ELBO)\n",
    "- `training_VAE` for training AE and VAE\n",
    "\n",
    "After you complete implementation, you are asked to train AE, VAE, and beta-VAE. Afterward, you can see the their generated result, loss curve, and some visualizations such as the latent space mapping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y5zGZaCvRLDF"
   },
   "source": [
    "## Exercise 1. Encoder\n",
    "In this exercise, you are asked to implment `Encoder` class. The `Encoder` class is composed of four convolutional layers and two fully-connected layers. The input of the `Encoder` class is an image tensor, and the output is a latent vector. The function you are asked to implement in the `Encoder` class is as follows:\n",
    "- \\_\\_init\\_\\_ : Define model structure with `self.model, self.fc_mean, self.fc_logvar`\n",
    "- reparameterize : (Only used in VAE) Reparameterize the latent vector with `mean` and `logvar`\n",
    "- forward : Forward propagation\n",
    "\n",
    "The Encoder could work as Encoder in AutoEncoder and Variational AutoEncoder. The operation depends on the argument `model_name` passed into the `Encoder` class.\n",
    "- If `model_name` is `AE`, the Encoder works as Encoder in AutoEncoder. \n",
    "- If `model_name` is `VAE`, the Encoder works as Encoder in Variational AutoEncoder.\n",
    "    - `beta-VAE` is also a variant of the VAE. Therefore, you can use the `VAE` model for `beta-VAE`.\n",
    "\n",
    "For VAE, you need to implement the `reparameterize` function in the `Encoder` class. The `reparameterize` function is used to reparameterize the latent vector. The latent vector is sampled from the Gaussian distribution with `mean` and `logvar`. The `reparameterize` function is defined as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "116iDI3mRLDF"
   },
   "source": [
    "### 1.1. \\_\\_init\\_\\_\n",
    "In this function, you are asked to implement the initialization of the `Encoder` class for AutoEncoder. The input of the `Encoder` class is an image tensor, and the output is a latent vector. The `input_dim` is the dimension of the input image, and the `hidden_dim` is the dimension of the hidden layer. The `latent_dim` is the dimension of the latent vector.\n",
    "### TODO\n",
    "- Implement the `__init__` function in `Encoder` class\n",
    "\n",
    "If you implement the `Encoder` class correctly, you can get the following result:\n",
    "\n",
    "For AE, the `__init__` function is as follows:\n",
    "```\n",
    "(model): Sequential(\n",
    "    (0): Conv2d(in_channels, hidden_dim[0], kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
    "    (1): LeakyReLU(negative_slope=0.01)\n",
    "    (2): Conv2d(hidden_dim[0], hidden_dim[1], kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
    "    (3): LeakyReLU(negative_slope=0.01)\n",
    "    (4): Conv2d(hidden_dim[1], hidden_dim[2], kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
    "    (5): LeakyReLU(negative_slope=0.01)\n",
    "    (6): Conv2d(hidden_dim[2], hidden_dim[3], kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
    "    (7): LeakyReLU(negative_slope=0.01)\n",
    "  )\n",
    "  (fc_mean): Linear(in_features=hidden_dim[3], out_features=2, bias=True)\n",
    "  (fc_logvar): Linear(in_features=hidden_dim[3], out_features=2, bias=True)\n",
    "```\n",
    "In Autoencoder, the `fc_mean` layer works as last layer and outputs the latent vector (`fc_logvar` is defined, but not used). Then, the latent vector directly passes to the `Decoder`.\n",
    "\n",
    "In Variational AutoEncoder, the outputs of `fc_mean` and `fc_logvar` layers are used to reparameterize the latent vector. Then, the reparameterized latent vector  passes to the `Decoder`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "- build the model structure with `self.model, self.fc_mean, self.fc_logvar` with given `in_channels`, `hidden_dim`, and `latent_dim`\n",
    "- ***You need to use `in_channels, hidden_dim, and latent_dim`*** which are passed to the Encoder. The architecture could be changed with the argument passed to the `Encoder` class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-12-01T17:20:06.440194Z",
     "iopub.status.busy": "2023-12-01T17:20:06.439806Z",
     "iopub.status.idle": "2023-12-01T17:20:07.116320Z",
     "shell.execute_reply": "2023-12-01T17:20:07.115683Z"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1701530542369,
     "user": {
      "displayName": "배현웅",
      "userId": "14643794117352663059"
     },
     "user_tz": -540
    },
    "id": "nI6txG5lRLDF",
    "outputId": "c05c90ce-7558-4905-f4a7-9631d1e417dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder(\n",
      "  (model): Sequential(\n",
      "    (0): Conv2d(1, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Conv2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Conv2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): Conv2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (7): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (fc_mean): Linear(in_features=32, out_features=2, bias=True)\n",
      "  (fc_logvar): Linear(in_features=32, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from HW4_1_YourAnswer import Encoder\n",
    "# for AE and VAE \n",
    "encoder = Encoder(hidden_dims = config.hidden_dims, \n",
    "                  latent_dim=config.latent_dim,model_name='AE').to(device) # or model_name='VAE'\n",
    "print(encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 forward for AutoEncoder\n",
    "In this function, you are asked to implement the forward path of the `Encoder` class. The forward path of the `Encoder` class is as follows:\n",
    "- Input image tensor is passed through the convolutional layers\n",
    "- The output of the convolutional layers is passed through the one fully-connected layers (`fc_mean`). \n",
    "- The output of the fully-connected layers (`self.fc_mean`) will be returned\n",
    "    - Need to flatten the output of the convolutional layers before passing through the fully-connected layers\n",
    "\n",
    "### TODO\n",
    "- Implement the `forward` function in `Encoder` class for AutoEncoder\n",
    "\n",
    "If you implement the `forward` function correctly, you can get the following output shape:\n",
    "- (batch_size, latent_dim)\n",
    "\n",
    "and you will get the following result:\n",
    "```\n",
    "    torch.Size([2, 2])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape=torch.Size([2, 32, 1, 1])\n",
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(hidden_dims = config.hidden_dims, \n",
    "                  latent_dim=config.latent_dim,model_name='AE').to(device) \n",
    "image = cv2.imread(\"./test_file/GAN_0050.png\",cv2.IMREAD_GRAYSCALE)[2:18,2:18]\n",
    "image = image/255\n",
    "image = torch.Tensor(image).unsqueeze(0).unsqueeze(0).repeat(2,1,1,1).to(device)\n",
    "out = encoder(image)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cTCPMro3RLDF"
   },
   "source": [
    "### 1.3 reparameterize trick for VAE\n",
    "The implementation of `Encoder` for `AE` is done. It is time to construct `Encoder` for `VAE`. For the first step, you need to implement the `reparameterize` function in the `Encoder` class.\n",
    "\n",
    "In this function, you are asked to implement the reparameterization trick. The reparameterization trick is as follows:\n",
    "- Sample a random vector $\\epsilon$ (in our code, `eps`) from the standard normal distribution\n",
    "- Compute the latent vector (in our code, `rp`) by $\\mu + \\sigma \\odot \\epsilon$, where $\\mu$ and $\\sigma$ are the mean and standard deviation of the latent distribution, respectively. $\\odot$ is the element-wise multiplication.\n",
    "- Return the latent vector `rp`\n",
    "\n",
    "### TODO\n",
    "- Implement the `reparameterize` function in `Encoder` class\n",
    "    - In our code, `mu` is the mean of the latent distribution, and `logvar` is the log standard deviation of the latent distribution.\n",
    "    - `eps` is the random vector sampled from the standard normal distribution and obtained from argument of the `reparameterize` function (Do not need to consider making it.).\n",
    "    \n",
    "If you implement the `reparameterize` function correctly, you can get the following result:\n",
    "```\n",
    "tensor([ 0.0051, -0.0005], device='cuda:0')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-12-01T17:20:07.118894Z",
     "iopub.status.busy": "2023-12-01T17:20:07.118700Z",
     "iopub.status.idle": "2023-12-01T17:20:07.134814Z",
     "shell.execute_reply": "2023-12-01T17:20:07.134258Z"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1701530542369,
     "user": {
      "displayName": "배현웅",
      "userId": "14643794117352663059"
     },
     "user_tz": -540
    },
    "id": "q6l6lF3PRLDF",
    "outputId": "4b162c5a-bd33-4222-8e1c-3a2cad201066"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0051, -0.0005])\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(hidden_dims = config.hidden_dims, latent_dim=config.latent_dim,\n",
    "                  model_name='VAE').to(device)\n",
    "eps = torch.Tensor([0.1,-0.01]).to(device)\n",
    "mu = torch.Tensor([-0.1,0.01]).to(device)\n",
    "logvar = torch.Tensor([0.1,0.1]).to(device)\n",
    "out = encoder.reparametrize(mu, logvar,eps)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OXCIjCFiRLDF"
   },
   "source": [
    "### 1.4 forward function for VAE\n",
    "In this function, you are asked to implement the forward path of the `Encoder` class for VAE. The forward path of the `Encoder` class is as follows:\n",
    "- Input image tensor is passed through the convolutional layers\n",
    "- The output of the convolutional layers is passed through the two fully-connected layers (`fc_mean`, `fc_logvar`), respectively.\n",
    "    - Need to flatten the output of the convolutional layers before passing through the fully-connected layers\n",
    "- The ***two output*** (`mu`,`logvar`) of the fully-connected layers is to through the `reparameterize` function\n",
    "\n",
    "### TODO\n",
    "- Implement the `forward` function in `Encoder` class\n",
    "\n",
    "If you implement the `forward` function correctly, you can get the following output shape:\n",
    "- `mean`: (batch_size, latent_dim)\n",
    "- `logvar`: (batch_size, latent_dim)\n",
    "- `rp`: (batch_size, latent_dim)\n",
    "\n",
    "and you will get the following result:\n",
    "```\n",
    "```\n",
    "    torch.Size([2, 2]) torch.Size([2, 2]) torch.Size([2, 2])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-12-01T17:20:07.137294Z",
     "iopub.status.busy": "2023-12-01T17:20:07.137093Z",
     "iopub.status.idle": "2023-12-01T17:20:08.542800Z",
     "shell.execute_reply": "2023-12-01T17:20:08.542151Z"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1701530543017,
     "user": {
      "displayName": "배현웅",
      "userId": "14643794117352663059"
     },
     "user_tz": -540
    },
    "id": "RGMuW4MBRLDF",
    "outputId": "68dc7008-a72e-441d-8ce6-81639bae1eb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape=torch.Size([2, 32, 1, 1])\n",
      "torch.Size([2, 2]) torch.Size([2, 2]) torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "image = cv2.imread(\"./test_file/GAN_0050.png\",cv2.IMREAD_GRAYSCALE)[2:18,2:18]\n",
    "image = image/255\n",
    "image = torch.Tensor(image).unsqueeze(0).unsqueeze(0).repeat(2,1,1,1).to(device)\n",
    "mean, logvar, rp = encoder(image)\n",
    "print(mean.shape, logvar.shape, rp.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_fMLALP7RLDG"
   },
   "source": [
    "## Exercise 2. Decoder\n",
    "In this exercise, you are asked to implment `Decoder` class. The `Decoder` class is composed of four convolutional layers. The `self.decoder` architecture is reverse version of `Encoder` except `fc_mean` and `fc_logvar` layer. The input of the `Encoder` class is an image tensor, and the output is a latent vector. The function you are asked to implement in the `Encoder` class is as follows:\n",
    "- \\_\\_init\\_\\_ : Define model structure with `self.input_layer, self.model,self.last_layer`\n",
    "- forward(self, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fl5g8od3RLDG"
   },
   "source": [
    "### 2.1. \\_\\_init\\_\\_\n",
    "In this exercise, you are asked to complete the initialization of the `Decoder` class. The `Decoder` class is composed of four convolutional layers. The architecture is reverse version of `Encoder`. The input of the `Encoder` class is an image tensor, and the output is a latent vector. The `input_dim` is the dimension of the input image, and the `hidden_dim` is the dimension of the hidden layer. The `latent_dim` is the dimension of the latent vector. \n",
    "\n",
    "`self.input_layer` works as the first layer of the `Decoder`. The output of the `self.input_layer` is reshaped to `(B,self.hidden_dims[-1],self.expand_dim,self.expand_dim)`. Then, the reshaped output is passed through the `self.decoder` layers. The output of the `self.decoder` layers is passed through the `self.last_layer` layer. The output of the `self.last_layer` layer is the output of the `Decoder`.\n",
    "\n",
    "### TODO\n",
    "- Implement the decoder model with `self.input_layer, self.decoder, self.last_layer` in `__init__` function of `Decoder` class\n",
    "- ***You need to use `latent_dim, hidden_dim, and expand_dim`*** which passed to the `Decoder` class. The architecture could be changed with the argument passed to the `Decoder` class.\n",
    "\n",
    "If you implement the `Decoder` class correctly, you can get the following result:\n",
    "```\n",
    "Decoder(\n",
    "  (input_layer): Linear(in_features=latent_dim, out_features=self.hidden_dims[3] * self.expand_dim**2 , bias=True)\n",
    "  (decoder): Sequential(\n",
    "    (0): ConvTranspose2d(self.hidden_dims[3], self.hidden_dims[2], kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
    "    (1): LeakyReLU(negative_slope=0.01)\n",
    "    (2): ConvTranspose2d( self.hidden_dims[2],  self.hidden_dims[1], kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
    "    (3): LeakyReLU(negative_slope=0.01)\n",
    "    (4): ConvTranspose2d( self.hidden_dims[1],  self.hidden_dims[0], kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
    "    (5): LeakyReLU(negative_slope=0.01)\n",
    "  )\n",
    "  (last_layer): Sequential(\n",
    "    (0): ConvTranspose2d(self.hidden_dims[0], 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (1): Sigmoid()\n",
    "  )\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-12-01T17:20:08.545531Z",
     "iopub.status.busy": "2023-12-01T17:20:08.545325Z",
     "iopub.status.idle": "2023-12-01T17:20:08.559656Z",
     "shell.execute_reply": "2023-12-01T17:20:08.559168Z"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1701530543017,
     "user": {
      "displayName": "배현웅",
      "userId": "14643794117352663059"
     },
     "user_tz": -540
    },
    "id": "xq5_ZH49xydT",
    "outputId": "140e61ad-6a3e-4733-b63f-82e081577cb9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[256, 128, 64, 32]\n",
      "Decoder(\n",
      "  (input_layer): Linear(in_features=2, out_features=128, bias=True)\n",
      "  (decoder): Sequential(\n",
      "    (0): ConvTranspose2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): ConvTranspose2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): ConvTranspose2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (last_layer): Sequential(\n",
      "    (0): ConvTranspose2d(256, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from HW4_1_YourAnswer import *\n",
    "print(config.hidden_dims)\n",
    "decoder = Decoder(hidden_dims = config.hidden_dims, latent_dim=config.latent_dim,expand_dim=config.expand_dim).to(device)\n",
    "print(decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WWfHXxjpRLDG"
   },
   "source": [
    "### 2.2 forward\n",
    "In this function, you are asked to implement the forward function of the `Decoder` class. The forward function of the `Decoder` class is as follows:\n",
    "- Input latent vector is passed through the `self.input_layer`\n",
    "- The output of the fully-connected layer is reshaped to `(B,self.hidden_dims[-1],self.expand_dim,self.expand_dim)`\n",
    "- The output of the fully-connected layer is passed through `self.decoder`\n",
    "- The output of the convolutional layers is passed through `self.last_layer`\n",
    "\n",
    "### TODO\n",
    "- Implement the forward path of the `Decoder` class\n",
    "\n",
    "If you implement the `forward` function correctly, you can get the following decoder output shape:\n",
    "```\n",
    "For AE, decoder output : torch.Size([2, 1, 16, 16])\n",
    "For VAE, decoder output : torch.Size([2, 1, 16, 16])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-12-01T17:20:08.562001Z",
     "iopub.status.busy": "2023-12-01T17:20:08.561820Z",
     "iopub.status.idle": "2023-12-01T17:20:08.573636Z",
     "shell.execute_reply": "2023-12-01T17:20:08.573152Z"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1701530543018,
     "user": {
      "displayName": "배현웅",
      "userId": "14643794117352663059"
     },
     "user_tz": -540
    },
    "id": "W0foE6_wRLDG",
    "outputId": "8bdd2b21-d6a4-4467-80f4-4c8a974b7307"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape=torch.Size([2, 32, 1, 1])\n",
      "For AE, decoder output : torch.Size([2, 1, 16, 16])\n",
      "x.shape=torch.Size([2, 32, 1, 1])\n",
      "For VAE, decoder output : torch.Size([2, 1, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "from HW4_1_YourAnswer import Encoder, Decoder\n",
    "decoder = Decoder(hidden_dims = config.hidden_dims, latent_dim=config.latent_dim,expand_dim=config.expand_dim).to(device)\n",
    "input = torch.randn(2, 1, 16, 16).to(device)\n",
    "# AE case\n",
    "encoder = Encoder(hidden_dims = config.hidden_dims, latent_dim=config.latent_dim,\n",
    "                  model_name='AE').to(device)\n",
    "latent = encoder(input)\n",
    "out = decoder(latent)\n",
    "print(\"For AE, decoder output :\",out.shape)\n",
    "# VAE case\n",
    "encoder = Encoder(hidden_dims = config.hidden_dims, latent_dim=config.latent_dim,\n",
    "                  model_name='VAE').to(device)\n",
    "mean, logvar, rp = encoder(input)\n",
    "out = decoder(rp)\n",
    "print(\"For VAE, decoder output :\",out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iGGTbGQKRLDG"
   },
   "source": [
    "## Exercise 3. Loss function (Evidence Lower Bound)\n",
    "In this exercise, you are asked to implement the loss function of the `AE` and `beta-VAE`. \n",
    "The `Encoder` and `Decoder` follows the below:\n",
    "- The output of the 'Encoder' class follows the `Gaussian distribution` for the `VAE` family and does not follow any distribution for the `AE`. \n",
    "- The output of the 'Decoder' class follows the `Bernoulli distribution`.\n",
    "\n",
    "Before you implement the loss function of the VAE, you need to implement the loss function of the AE. \n",
    "\n",
    "The loss function of the AE is as follows:\n",
    "- AE loss\n",
    "    - $L_{AE} = L_{rec}$\n",
    "\n",
    "Where $L_{rec}$ is reconstruction loss (same loss for VAE and beta-VAE).\n",
    "\n",
    "And, the loss function of the beta-VAE is as follows:\n",
    "- VAE loss\n",
    "    - $L_{VAE} = L_{rec} + \\beta L_{KL}$\n",
    "\n",
    "    \n",
    "Where $L_{rec}$ is reconstruction loss and $L_{KL}$ is KL divergence loss. $\\beta$ is the hyperparameter that balances the latent channel capacity and independence constraints with reconstruction accuracy.\n",
    "\n",
    "When $\\beta$ is 1, the loss function is the same as the VAE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wa-HPQQcRLDG"
   },
   "source": [
    "### TODO\n",
    "- Implement `reconstruction loss` function for $L_{rec}$\n",
    "    - Sum over every pixels and average for each batch\n",
    "- Implement `KLD_loss` function (KL divergence loss) for $L_{KL}$\n",
    "    - Do not average, just sum the loss\n",
    "- Implement `loss_function` for training VAE\n",
    "- ***Write you own code***\n",
    "    - Do not use the function in `torch.nn.functional` or other `torch` package function which directly compute the loss. \n",
    "    \n",
    "Depending on the your implementation, it might be different from the result of the `torch` package function. However, ***please follow the above loss function instruction and standard formulation for these losses.***\n",
    "If you implement the `loss_function` correctly, you can get the following result without error (small difference is allowed if no error is occured):\n",
    "```\n",
    "======== your implementation outcome is\n",
    "- loss 191.0272216796875\n",
    "- recon_loss 191.01107788085938\n",
    "- kld_loss 0.016147971153259277\n",
    "======== The results should be\n",
    "- loss 191.0272216796875\n",
    "- recon_loss 191.01107788085938\n",
    "- kld_loss 0.016147971153259277\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-12-01T17:20:08.576084Z",
     "iopub.status.busy": "2023-12-01T17:20:08.575887Z",
     "iopub.status.idle": "2023-12-01T17:20:08.604146Z",
     "shell.execute_reply": "2023-12-01T17:20:08.603495Z"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1701530543018,
     "user": {
      "displayName": "배현웅",
      "userId": "14643794117352663059"
     },
     "user_tz": -540
    },
    "id": "V3DDiXqDRLDG",
    "outputId": "f44af088-9061-414f-a1ad-6c6d24243815"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== your implementation outcome is\n",
      "- loss 191.0272674560547\n",
      "- recon_loss 191.01112365722656\n",
      "- kld_loss 0.016147911548614502\n",
      "======== The results should be\n",
      "- loss 191.0272216796875\n",
      "- recon_loss 191.01107788085938\n",
      "- kld_loss 0.016147971153259277\n"
     ]
    }
   ],
   "source": [
    "from HW4_1_YourAnswer import *\n",
    "from utils import *\n",
    "import cv2\n",
    "config.latent_dim = 2\n",
    "image1 = cv2.imread(\"./test_file/GAN_0050.png\",cv2.IMREAD_GRAYSCALE)[2:18,2:18] / 255\n",
    "image1 = torch.Tensor(image1).unsqueeze(0).unsqueeze(0).repeat(2,1,1,1).to(device)\n",
    "image2 = cv2.imread(\"./test_file/GAN_0100.png\",cv2.IMREAD_GRAYSCALE)[2:18,2:18] / 255\n",
    "image2 = torch.Tensor(image2).unsqueeze(0).unsqueeze(0).repeat(2,1,1,1).to(device)\n",
    "\n",
    "encoder = Encoder(hidden_dims = config.hidden_dims, latent_dim=config.latent_dim).to(device)\n",
    "decoder = Decoder(hidden_dims = config.hidden_dims, latent_dim=config.latent_dim,expand_dim=config.expand_dim).to(device)\n",
    "# If you have trouble loading the model, please check the model architecture\n",
    "encoder.load_state_dict(torch.load('./test_file/init_encoder.pth',map_location=device))\n",
    "decoder.load_state_dict(torch.load('./test_file/init_decoder.pth',map_location=device))\n",
    "\n",
    "\n",
    "eps = torch.Tensor([0.1,-0.01]).to(device)\n",
    "mean1, logvar1, rp1 = encoder(image1,eps)\n",
    "\n",
    "recon_image1 = decoder(rp1)\n",
    "result = loss_function(recon_image1,image1,mean1,logvar1,return_info=True)\n",
    "\n",
    "reference_result = torch.load('./test_file/test_results_VAE_loss.pth',map_location=device)\n",
    "\n",
    "print(\"======== your implementation outcome is\")\n",
    "for key, value in result.items():\n",
    "    print(\"-\",key, value.item())\n",
    "print(\"======== The results should be\")\n",
    "for key, value in reference_result.items():\n",
    "    print(\"-\",key, value.item())\n",
    "\n",
    "assert abs(result['loss'].item() - reference_result['loss'].item()) < 1e-2, f\"Wrong loss function implementation {result['loss'].item()}\"\n",
    "assert abs(result['recon_loss'].item() - reference_result['recon_loss'].item()) < 1e-2, f\"Wrong reconstruction loss function implementation {result['recon_loss'].item()}\"\n",
    "assert abs(result['kld_loss'].item() - reference_result['kld_loss'].item()) < 1e-2, f\"Wrong KLD loss function implementation {result['kld_loss'].item()}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N6biCOm4nCE0"
   },
   "source": [
    "To check the correctness of the `loss function`, change beta value to 1,5,10 and check the result. If you implement the `loss function` correctly, you can get the following result:\n",
    "```\n",
    "    beta 1\t: 48.04874801635742\n",
    "    beta 5\t: 48.07823181152344\n",
    "    beta 10\t: 48.115089416503906\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-12-01T17:20:08.607203Z",
     "iopub.status.busy": "2023-12-01T17:20:08.606998Z",
     "iopub.status.idle": "2023-12-01T17:20:08.619708Z",
     "shell.execute_reply": "2023-12-01T17:20:08.619242Z"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1701530543018,
     "user": {
      "displayName": "배현웅",
      "userId": "14643794117352663059"
     },
     "user_tz": -540
    },
    "id": "lEhWslNEnCE0",
    "outputId": "1e8fe1bc-7f61-4c2a-943e-6b1c35467a9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta 1\t: 48.04875183105469\n",
      "beta 5\t: 48.0782356262207\n",
      "beta 10\t: 48.11509323120117\n"
     ]
    }
   ],
   "source": [
    "mu = torch.Tensor([[-0.1,0.01],[0.05,-0.01]]).to(device)\n",
    "logvar = torch.Tensor([[0.1,0.1],[0.05,0.1]]).to(device)\n",
    "for beta in [1,5,10]:\n",
    "    loss = loss_function(image1,image1,mu,logvar,beta=beta)\n",
    "    print(f\"beta {beta}\\t:\",loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pIHc1CqtRLDG"
   },
   "source": [
    "## Exercise 4. Training beta-VAE\n",
    "In this exercise, you are asked to implement the training code of the beta-VAE. The training flow of the beta-VAE is as follows (Same as the VAE):\n",
    "- Input image tensor is passed through the `Encoder` class\n",
    "- The output of the `Encoder` class is passed through the `Decoder` class\n",
    "- The outputs of the `Decoder` class and `Encoder` class are compared with the input image tensor using the `loss function`.\n",
    "- The loss is backpropagated to update the parameters of the `Encoder` and `Decoder` classes\n",
    "- Update the parameters of the `Encoder` and `Decoder` classes\n",
    "\n",
    "For training, you need to implement the following functions and classes:\n",
    "- Define dataloader in the `__init__` function of the `dataloader` class\n",
    "- Define `self.optimizer` in the `__init__` function of the `training_VAE` class\n",
    "- Implement `one_iter_train` of the VAE in the `training_VAE` class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9wvXLGiNnCE1"
   },
   "source": [
    "### 4.1. Implement dataloader for training procedure\n",
    "In this exercise, you are asked to implement the `dataloader` class. The `dataloader` class is used to load the MNIST dataset. \n",
    "\n",
    "To implement dataloader, you need to define `preprocessing` and `dataloader` for dataset. The implementation sequence is as below:\n",
    "1. implement `preprocessing` via `torchvision.transforms`\n",
    "    - The `preprocessing` consists some transformation or processsing before loading on the network, such as normalization, resize, or other data augmentation.\n",
    "2. Declare `dataset` via `torchvision.datasets`.\n",
    "    - In VAE, you are asked to download and load the `MNIST`from the online.\n",
    "    - In this seqeuence, the `preprocessing` is applied with `transform` argument.\n",
    "3. Create `dataloader` which directly feeds the image into GAN networks.\n",
    "    - It defines batch size, shuffling option, or etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3W25XWCEnCE1"
   },
   "source": [
    "### 4.1.1 Preprocessing the dataset\n",
    "In this exercise, we implement the transformation function via `torchvision.transforms`.\n",
    "The preprocessing for the input consists of 3 components:\n",
    "1. Resize the image\n",
    "    - The expected image shape for the generator and discriminator  is (1,16,16), but the original image shape of MNIST is (1,28,28). To align it, the input image should be `resize` into (1,16,16).\n",
    "2. Change the data type\n",
    "    - You need to `change variable type` from numpy array `into torch.Tensor` to train neural network since the network expects torch.Tensor type input.\n",
    "    \n",
    "For more details, see the [link](https://pytorch.org/vision/0.9/transforms.html).\n",
    "\n",
    "### TODO\n",
    "- Implement transformation (preprocessing) for the input image in `dataloader` class (The code in `HW4_1_YourAnswer.py`)\n",
    "\n",
    "If implemented correctly, the results will as shown:\n",
    "```\n",
    "Compose(\n",
    "    Resize(size=16, interpolation=bilinear, max_size=None, antialias=None)\n",
    "    ToTensor()\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-12-01T17:20:08.622203Z",
     "iopub.status.busy": "2023-12-01T17:20:08.622008Z",
     "iopub.status.idle": "2023-12-01T17:20:08.635150Z",
     "shell.execute_reply": "2023-12-01T17:20:08.634619Z"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1701530543018,
     "user": {
      "displayName": "배현웅",
      "userId": "14643794117352663059"
     },
     "user_tz": -540
    },
    "id": "FopzsMfenCE1",
    "outputId": "f62cc265-3299-4884-9962-cd506b344429"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████| 9912422/9912422 [00:00<00:00, 11250379.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████| 28881/28881 [00:00<00:00, 2626988.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████| 1648877/1648877 [00:00<00:00, 4054027.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 4542/4542 [00:00<00:00, 11245884.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Compose(\n",
      "    Resize(size=16, interpolation=bilinear, max_size=None, antialias=None)\n",
      "    ToTensor()\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from HW4_1_YourAnswer import *\n",
    "loader_for_test = dataloader(train=False, batch_size=config.batch_size)\n",
    "print(loader_for_test.transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dld0rjEJnCE1"
   },
   "source": [
    "### 4.1.2 Implement dataset and dataloader\n",
    "In this practice, you are asked to implement dataset and dataloader.\n",
    "### TODO\n",
    "- Implement self.dataset to load MNIST dataset using `torchvision.datasets`. (The code in `HW4_1_YourAnswer.py`)\n",
    "- Implement self.dataloader (The code in `HW4_1_YourAnswer.py`)\n",
    "\n",
    "If you implemented correctly, the output will be as below (min / max value might be different, just check the value is in the range of 0~1):\n",
    "```\n",
    "The shape of image from the dataloader : torch.Size([10, 1, 16, 16])\n",
    "The type of image from the dataloader : <class 'torch.Tensor'>\n",
    "The min/max of image from the dataloader : 0.0 / 0.9921568632125854\n",
    "The shape of label from the dataloader : torch.Size([10])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-12-01T17:20:08.637492Z",
     "iopub.status.busy": "2023-12-01T17:20:08.637093Z",
     "iopub.status.idle": "2023-12-01T17:20:08.652794Z",
     "shell.execute_reply": "2023-12-01T17:20:08.652277Z"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1701530543018,
     "user": {
      "displayName": "배현웅",
      "userId": "14643794117352663059"
     },
     "user_tz": -540
    },
    "id": "zO85wVf8nCE1",
    "outputId": "f8eb401c-c96b-4db5-b39b-9705fc4bb190"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of image from the dataloader : torch.Size([10, 1, 16, 16])\n",
      "The type of image from the dataloader : <class 'torch.Tensor'>\n",
      "The min / max of image from the dataloader : 0.0 / 0.9921568632125854\n",
      "The shape of label from the dataloader : torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "from HW4_1_YourAnswer import *\n",
    "loader_for_test = dataloader(train=False, batch_size=10)\n",
    "for img, label in loader_for_test:\n",
    "    print(f\"The shape of image from the dataloader : {img.shape}\")\n",
    "    print(f\"The type of image from the dataloader : {type(img)}\")\n",
    "    print(f\"The min / max of image from the dataloader : {img.min()} / {img.max()}\")\n",
    "    print(f\"The shape of label from the dataloader : {label.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eWQWoDiNnCE1"
   },
   "source": [
    "If implemented correctly, the hand-written digit image will be shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "execution": {
     "iopub.execute_input": "2023-12-01T17:20:08.654840Z",
     "iopub.status.busy": "2023-12-01T17:20:08.654720Z",
     "iopub.status.idle": "2023-12-01T17:20:08.947589Z",
     "shell.execute_reply": "2023-12-01T17:20:08.947050Z"
    },
    "executionInfo": {
     "elapsed": 576,
     "status": "ok",
     "timestamp": 1701530543591,
     "user": {
      "displayName": "배현웅",
      "userId": "14643794117352663059"
     },
     "user_tz": -540
    },
    "id": "HL2ymdzRnCE1",
    "outputId": "3de2e866-6201-42f2-b250-fe5e6a8f19da"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "img = img.numpy().transpose(0,2,3,1)\n",
    "label = label.numpy().squeeze()\n",
    "for i in range(10):\n",
    "  train_x = img[i]\n",
    "  train_y = label[i]\n",
    "\n",
    "  ax = fig.add_subplot(1, 10, i+1)\n",
    "  ax.imshow(train_x)\n",
    "  ax.set_title(str(train_y.item()))\n",
    "  ax.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FIrfhNGZnCE1"
   },
   "source": [
    "### 4.2 Training Sequence\n",
    "In this section, you are asked to implement the training sequence of the VAE. \n",
    "\n",
    "### TODO\n",
    "- Implement `self.optimizer` in the `__init__` function of the `training_VAE` class\n",
    "    - You can use `Adam` optimizer with `config.lr` arguments\n",
    "- Complete the `one_iter_train` function in the `training_VAE` class.\n",
    "    - In the `one_iter_train` function, you should covers two cases: `AE` and `VAE` with `self.model_name` defined in the `__init__` function of the `training_VAE` class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-SSjQwjBRLDG"
   },
   "source": [
    "If training sequence of AE is implemented correctly, the below cell is executed without error and print the following result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "your implementation outcome is\n",
      " 196.13107299804688\n",
      "The results should be\n",
      " 196.1310272216797\n"
     ]
    }
   ],
   "source": [
    "from HW4_1_YourAnswer import *\n",
    "from copy import deepcopy\n",
    "# for AE\n",
    "test_config = deepcopy(config)\n",
    "test_config.batch_size =2\n",
    "test_config.latent_dim = 2\n",
    "eps = torch.load('./test_file/test_eps.pth',map_location=device)\n",
    "images = torch.load('./test_file/test_images_MNIST.pth',map_location=device)\n",
    "labels = torch.load('./test_file/test_labels_MNIST.pth',map_location=device)\n",
    "\n",
    "encoder = Encoder(hidden_dims = test_config.hidden_dims, latent_dim=test_config.latent_dim,model_name='AE').to(device)\n",
    "decoder = Decoder(hidden_dims = test_config.hidden_dims, latent_dim=test_config.latent_dim,expand_dim=test_config.expand_dim).to(device)\n",
    "# If you have trouble loading the model, please check the model architecture\n",
    "encoder.load_state_dict(torch.load('./test_file/init_encoder.pth',map_location=device),strict=False)\n",
    "decoder.load_state_dict(torch.load('./test_file/init_decoder.pth',map_location=device),strict=False)\n",
    "\n",
    "trainer = training_VAE(train_loader = None, test_loader = None,\n",
    "                       encoder=encoder, decoder=decoder,\n",
    "                       device=device, config=test_config, save_img=False,\n",
    "                       model_name='AE')\n",
    "result = trainer.one_iter_train(images,labels,eps)\n",
    "reference_result = torch.load('./test_file/test_results_AE.pth',map_location=device)\n",
    "print(\"your implementation outcome is\\n\",result['recon_loss'])\n",
    "print(\"The results should be\\n\",reference_result['recon_loss'])\n",
    "\n",
    "assert abs(result['recon_loss'] - reference_result['recon_loss']) < 1e-2, \"Reconstruction loss is wrong\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If training sequence of VAE is implemented correctly, the below cell is executed without error and print the following result:\n",
    "```\n",
    "your implementation outcome is\n",
    " {'recon_loss': 195.93630981445312, 'kld_loss': 0.015837177634239197}\n",
    "The results should be\n",
    " {'recon_loss': 195.93630981445312, 'kld_loss': 0.015837177634239197}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-12-01T17:20:08.949990Z",
     "iopub.status.busy": "2023-12-01T17:20:08.949807Z",
     "iopub.status.idle": "2023-12-01T17:20:09.082308Z",
     "shell.execute_reply": "2023-12-01T17:20:09.081594Z"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1701530543592,
     "user": {
      "displayName": "배현웅",
      "userId": "14643794117352663059"
     },
     "user_tz": -540
    },
    "id": "zdo4tInZRLDG",
    "outputId": "64fc2e40-6fd0-4b6a-c1fb-df5b78c82f92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "your implementation outcome is\n",
      " {'recon_loss': 195.9363250732422, 'kld_loss': 0.01583714783191681}\n",
      "The results should be\n",
      " {'recon_loss': 195.93630981445312, 'kld_loss': 0.015837177634239197}\n"
     ]
    }
   ],
   "source": [
    "from HW4_1_YourAnswer import *\n",
    "from copy import deepcopy\n",
    "\n",
    "# training VAE\n",
    "test_config = deepcopy(config)\n",
    "test_config.batch_size =2\n",
    "test_config.latent_dim = 2\n",
    "eps = torch.load('./test_file/test_eps.pth',map_location=device)\n",
    "images = torch.load('./test_file/test_images_MNIST.pth',map_location=device)\n",
    "labels = torch.load('./test_file/test_labels_MNIST.pth',map_location=device)\n",
    "\n",
    "encoder = Encoder(hidden_dims = test_config.hidden_dims, latent_dim=test_config.latent_dim,model_name='VAE').to(device)\n",
    "decoder = Decoder(hidden_dims = test_config.hidden_dims, latent_dim=test_config.latent_dim,expand_dim=test_config.expand_dim).to(device)\n",
    "# If you have trouble loading the model, please check the model architecture\n",
    "encoder.load_state_dict(torch.load('./test_file/init_encoder.pth',map_location=device))\n",
    "decoder.load_state_dict(torch.load('./test_file/init_decoder.pth',map_location=device))\n",
    "\n",
    "trainer = training_VAE(train_loader = None, test_loader = None,\n",
    "                       encoder=encoder, decoder=decoder,\n",
    "                       device=device, config=test_config, save_img=False)\n",
    "result = trainer.one_iter_train(images,labels,eps)\n",
    "\n",
    "reference_result = torch.load('./test_file/test_results_VAE.pth',map_location=device)\n",
    "print(\"your implementation outcome is\\n\",result)\n",
    "print(\"The results should be\\n\",reference_result)\n",
    "\n",
    "assert abs(result['recon_loss'] - reference_result['recon_loss']) < 1e-2, \"Reconstruction loss is wrong\"\n",
    "assert abs(result['kld_loss'] - reference_result['kld_loss']) < 1e-2, \"KL divergence loss is wrong\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Training AE, VAE, and beta-VAE\n",
    "With your implementation, you can train AE, VAE, and beta-VAE. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kb9iuwLiRLDG"
   },
   "source": [
    "### Let's train AE!\n",
    "Try to train AE with your own implemented code. You can train 20 epochs to get the reasonable result (It takes around 10 minutes to train 20 epoch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                   | 0/234 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'item'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m decoder\u001b[38;5;241m.\u001b[39mapply(initialize_weights)\n\u001b[1;32m     11\u001b[0m trainer \u001b[38;5;241m=\u001b[39m training_VAE(train_loader, test_loader,encoder, decoder, device, config, \\\n\u001b[1;32m     12\u001b[0m                        save_img\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAE\u001b[39m\u001b[38;5;124m'\u001b[39m,beta\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mbeta,img_show\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 13\u001b[0m results_AE \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/intro_dl/hw4/HW4_1_YourAnswer.py:524\u001b[0m, in \u001b[0;36mtraining_VAE.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m--> 524\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mone_iter_train(images,labels,eps)\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m    526\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/intro_dl/hw4/HW4_1_YourAnswer.py:491\u001b[0m, in \u001b[0;36mtraining_VAE.one_iter_train\u001b[0;34m(self, images, label, eps)\u001b[0m\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    486\u001b[0m \u001b[38;5;66;03m############### YOUR CODE HERE ###############\u001b[39;00m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;66;03m##############################################\u001b[39;00m\n\u001b[1;32m    489\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    490\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecon_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m : recon_loss\u001b[38;5;241m.\u001b[39mitem(),\n\u001b[0;32m--> 491\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkld_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m : kld_loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    492\u001b[0m         }\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'item'"
     ]
    }
   ],
   "source": [
    "from HW4_1_YourAnswer import *\n",
    "from utils import initialize_weights\n",
    "\n",
    "train_loader = dataloader(train=True, batch_size=config.batch_size)\n",
    "test_loader = dataloader(train=False, batch_size=config.batch_size)\n",
    "encoder = Encoder(hidden_dims = config.hidden_dims, latent_dim=config.latent_dim,\n",
    "                  model_name='AE').to(device)\n",
    "decoder = Decoder(hidden_dims = config.hidden_dims, latent_dim=config.latent_dim,expand_dim=config.expand_dim).to(device)\n",
    "encoder.apply(initialize_weights)\n",
    "decoder.apply(initialize_weights)\n",
    "trainer = training_VAE(train_loader, test_loader,encoder, decoder, device, config, \\\n",
    "                       save_img=True,model_name='AE',beta=config.beta,img_show=True)\n",
    "results_AE = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_encoder_AE = results_AE['encoder']\n",
    "trained_decoder_AE = results_AE['decoder']\n",
    "recon_loss_AE = results_AE['Recon_loss_history']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Loss curve of AE\")\n",
    "plt.plot(recon_loss_AE, label='Recon_loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's train VAE!\n",
    "Try to train VAE with your own implemented code (with beta 1). You can train 20 epochs to get the reasonable result. (It takes around 10 minutes to train 20 epoch.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HW4_1_YourAnswer import *\n",
    "from utils import initialize_weights\n",
    "config.beta = 1\n",
    "train_loader = dataloader(train=True, batch_size=config.batch_size)\n",
    "test_loader = dataloader(train=False, batch_size=config.batch_size)\n",
    "encoder = Encoder(hidden_dims = config.hidden_dims, latent_dim=config.latent_dim,model_name='VAE').to(device)\n",
    "decoder = Decoder(hidden_dims = config.hidden_dims, latent_dim=config.latent_dim,expand_dim=config.expand_dim).to(device)\n",
    "encoder.apply(initialize_weights)\n",
    "decoder.apply(initialize_weights)\n",
    "trainer = training_VAE(train_loader, test_loader,encoder, decoder, device, config, \\\n",
    "                       save_img=True,model_name='VAE',beta=config.beta,img_show=True)\n",
    "results_VAE = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_encoder_VAE = results_VAE['encoder']\n",
    "trained_decoder_VAE = results_VAE['decoder']\n",
    "recon_loss_VAE= results_VAE['Recon_loss_history']\n",
    "KLD_loss_VAE= results_VAE['KLD_loss_history']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Loss curve of VAE\")\n",
    "plt.plot(recon_loss_VAE, label='Recon_loss')\n",
    "plt.plot(KLD_loss_VAE, label='KLD_loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's train beta-VAE!\n",
    "Try to train beta-VAE with your own implemented code (with beta 2). You can train 20 epochs to get the reasonable result. (It takes around 10 minutes to train 20 epoch.) If you want to get the results depending on the beta, you can change the beta value in the `training_VAE` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "execution": {
     "iopub.execute_input": "2023-12-01T17:20:09.085288Z",
     "iopub.status.busy": "2023-12-01T17:20:09.084743Z",
     "iopub.status.idle": "2023-12-01T17:21:40.524237Z",
     "shell.execute_reply": "2023-12-01T17:21:40.523591Z"
    },
    "executionInfo": {
     "elapsed": 205101,
     "status": "ok",
     "timestamp": 1701530748689,
     "user": {
      "displayName": "배현웅",
      "userId": "14643794117352663059"
     },
     "user_tz": -540
    },
    "id": "n5y2NtibxydU",
    "outputId": "81f89d18-3164-4be3-84f2-cf78f756518b"
   },
   "outputs": [],
   "source": [
    "from HW4_1_YourAnswer import *\n",
    "from utils import initialize_weights\n",
    "config.beta = 2\n",
    "train_loader = dataloader(train=True, batch_size=config.batch_size)\n",
    "test_loader = dataloader(train=False, batch_size=config.batch_size)\n",
    "encoder = Encoder(hidden_dims = config.hidden_dims, latent_dim=config.latent_dim,model_name='VAE').to(device)\n",
    "decoder = Decoder(hidden_dims = config.hidden_dims, latent_dim=config.latent_dim,expand_dim=config.expand_dim).to(device)\n",
    "encoder.apply(initialize_weights)\n",
    "decoder.apply(initialize_weights)\n",
    "trainer = training_VAE(train_loader, test_loader,encoder, decoder, device, config, \\\n",
    "                       save_img=True,model_name='beta_VAE',beta=config.beta,img_show=True)\n",
    "results_beta_VAE = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T17:21:40.526247Z",
     "iopub.status.busy": "2023-12-01T17:21:40.526014Z",
     "iopub.status.idle": "2023-12-01T17:21:40.538371Z",
     "shell.execute_reply": "2023-12-01T17:21:40.537933Z"
    },
    "id": "P1FSosifRLDH"
   },
   "outputs": [],
   "source": [
    "trained_encoder_beta_VAE = results_beta_VAE['encoder']\n",
    "trained_decoder_beta_VAE = results_beta_VAE['decoder']\n",
    "recon_loss_beta_VAE= results_beta_VAE['Recon_loss_history']\n",
    "KLD_loss_beta_VAE= results_beta_VAE['KLD_loss_history']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Loss curve of beta-VAE\")\n",
    "plt.plot(recon_loss_beta_VAE, label='Recon_loss')\n",
    "plt.plot(KLD_loss_beta_VAE, label='KLD_loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HymtmtE9RLDH"
   },
   "source": [
    "## Visualization of the training result\n",
    "If you implement the `training_VAE` class correctly, you can get the following result:\n",
    "- The generated image of the AE, VAE, and beta-VAE from same images\n",
    "- The reconstructed image of the AE, VAE, and beta-VAE from same images\n",
    "- The representation of the output with the change of latent vector\n",
    "- Visualization of the latent space mapping of the VAE and AutoEncoder\n",
    "- Visualization of the output image depending on the latent vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HW4_1_YourAnswer import dataloader\n",
    "import math\n",
    "test_loader = dataloader(train=False, batch_size=64)\n",
    "num_col=8\n",
    "for batch_idx, (image, label) in enumerate(test_loader):\n",
    "    image = (image).to(device)\n",
    "    param = trained_encoder_AE(image)\n",
    "    output_AE = trained_decoder_AE(param)\n",
    "    grid_output_AE = torchvision.utils.make_grid(output_AE, nrow=num_col, normalize=True).cpu().detach().permute(1,2,0).numpy()\n",
    "    mu, log_var, reparam = trained_encoder_VAE(image)\n",
    "    output_VAE = trained_decoder_VAE(reparam)\n",
    "    grid_output_VAE = torchvision.utils.make_grid(output_VAE, nrow=num_col, normalize=True).cpu().detach().permute(1,2,0).numpy()\n",
    "    mu, log_var, beta_reparam = trained_encoder_beta_VAE(image)\n",
    "    output_beta_VAE = trained_decoder_beta_VAE(beta_reparam)\n",
    "    grid_output_beta_VAE = torchvision.utils.make_grid(output_beta_VAE, nrow=num_col, normalize=True).cpu().detach().permute(1,2,0).numpy()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the generated images of the AE, VAE, and beta-VAE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1,3,1)\n",
    "plt.title(\"AE\")\n",
    "plt.imshow(grid_output_AE)\n",
    "plt.subplot(1,3,2)\n",
    "plt.title(\"VAE\")\n",
    "plt.imshow(grid_output_VAE)\n",
    "plt.subplot(1,3,3)\n",
    "plt.title(f\"beta-VAE (beta = {config.beta})\")\n",
    "plt.imshow(grid_output_beta_VAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fpZYZX6kRLDH"
   },
   "source": [
    "### The reconstructed image of the AE, VAE, and beta-VAE from same images\n",
    "If you implement the `training_VAE` class correctly, you can see the input image and the output image of the VAE. The input image is the leftmost image, and the output image is the 2nd,3rd and 4th image from left. It might fail to reconstruct the input image due to insufficient training, but you can see that the output image is similar to the input image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "execution": {
     "iopub.execute_input": "2023-12-01T17:21:40.659310Z",
     "iopub.status.busy": "2023-12-01T17:21:40.659189Z",
     "iopub.status.idle": "2023-12-01T17:21:41.130337Z",
     "shell.execute_reply": "2023-12-01T17:21:41.129836Z"
    },
    "executionInfo": {
     "elapsed": 512,
     "status": "ok",
     "timestamp": 1701530750020,
     "user": {
      "displayName": "배현웅",
      "userId": "14643794117352663059"
     },
     "user_tz": -540
    },
    "id": "yuYJjuCExydU",
    "outputId": "da3279d9-d58b-416f-868e-230f8751a290"
   },
   "outputs": [],
   "source": [
    "output = {'AE':output_AE.detach().cpu().numpy(),\n",
    "          'VAE':output_VAE.detach().cpu().numpy(),\n",
    "          'beta_VAE':output_beta_VAE.detach().cpu().numpy()}\n",
    "show_image_real_and_VAE(image,output,num_img=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cLXiAYCcRLDK"
   },
   "source": [
    "### The representation of the output with the change of latent vector\n",
    "In the following figure, you will see the visualization of the output image depending on the latent vector. Since the AE does not consider the distribution of the latent vector, the output image sometimes cannot produces image depending on the latent vector. On the other hand, the VAE considers the distribution of the latent vector, so the output image is changed depending on the latent vector. The image will be changed smoothly because the latent vector is sampled from the Gaussian distribution. \n",
    "\n",
    "The generated image is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AE_img = cv2.imread(\"./test_file/AE_latent_vector_img.png\")\n",
    "VAE_img = cv2.imread(\"./test_file/VAE_latent_vector_img.png\")\n",
    "beta_VAE_img = cv2.imread(\"./test_file/beta_VAE_latent_vector_img.png\")\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,3,1)\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(AE_img)\n",
    "plt.subplot(1,3,2)\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(VAE_img)\n",
    "plt.subplot(1,3,3)\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(beta_VAE_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "execution": {
     "iopub.execute_input": "2023-12-01T17:21:41.132801Z",
     "iopub.status.busy": "2023-12-01T17:21:41.132613Z",
     "iopub.status.idle": "2023-12-01T17:21:42.658393Z",
     "shell.execute_reply": "2023-12-01T17:21:42.657975Z"
    },
    "executionInfo": {
     "elapsed": 3340,
     "status": "ok",
     "timestamp": 1701530753356,
     "user": {
      "displayName": "배현웅",
      "userId": "14643794117352663059"
     },
     "user_tz": -540
    },
    "id": "d33hFHYCxydU",
    "outputId": "bb041567-c6ec-4892-fc6e-4121830aee56"
   },
   "outputs": [],
   "source": [
    "config.latent_dim = 2\n",
    "show_image_VAE('AE',trained_decoder_AE,config.latent_dim,size=15)\n",
    "plt.pause(0.01)\n",
    "show_image_VAE('VAE',trained_decoder_VAE,config.latent_dim,size=15)\n",
    "plt.pause(0.01)\n",
    "show_image_VAE('beta-VAE',trained_decoder_beta_VAE,config.latent_dim,size=15)\n",
    "plt.pause(0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ltVmTKGLnCE2"
   },
   "source": [
    "# Visualization of the latent vector of the VAE and AE\n",
    "In this section, you are asked to visualize the latent vector of the VAE and AE with your implementation. Just run and see the visualization. The visualization of the latent vector is performed to compare the latent vector of the VAE and AE.\n",
    "\n",
    "For comparison, we will use the trained AE and VAE with same structure. \n",
    "\n",
    "When you get the figure, the latent vector of the VAE and AE is visualized. The latent vector of the VAE is on the left, and the latent vector of the AE is on the right.\n",
    "\n",
    "The latent vector of the VAE is more evenly distributed and centered among origin point than the latent vector of the AE. This is because the VAE is trained with the KL divergence loss, which makes the latent vector evenly distributed. Therefore, it has benefit to generate more diverse images. It leads to understanding the latent vector of the VAE is more interpretable than the AE latent vector. Due to insufficient training, the latent vector of the AE and VAE might be quite far from the wanted distribution.. However, you can see the difference between the latent vector of the VAE and AE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 890
    },
    "execution": {
     "iopub.execute_input": "2023-12-01T17:21:43.499658Z",
     "iopub.status.busy": "2023-12-01T17:21:43.499467Z",
     "iopub.status.idle": "2023-12-01T17:21:44.068345Z",
     "shell.execute_reply": "2023-12-01T17:21:44.067759Z"
    },
    "executionInfo": {
     "elapsed": 2077,
     "status": "ok",
     "timestamp": 1701530756562,
     "user": {
      "displayName": "배현웅",
      "userId": "14643794117352663059"
     },
     "user_tz": -540
    },
    "id": "Msjb6RLUnCE2",
    "outputId": "d789d588-0d2c-4ee8-d109-5354f08942dc"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "output = cv2.imread(\"./test_file/VAE_AE_output.png\")\n",
    "plt.axis('off')\n",
    "plt.imshow(output)\n",
    "plt.pause(0.01)\n",
    "output = cv2.imread(\"./test_file/VAE_AE_output_1.png\")\n",
    "plt.axis('off')\n",
    "plt.imshow(output)\n",
    "plt.pause(0.01)\n",
    "output = cv2.imread(\"./test_file/VAE_AE_output_2.png\")\n",
    "plt.axis('off')\n",
    "plt.imshow(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iHX8RPE_nCE2"
   },
   "source": [
    "### Let's compare the latent vector of the VAE and AE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "execution": {
     "iopub.execute_input": "2023-12-01T17:21:44.070893Z",
     "iopub.status.busy": "2023-12-01T17:21:44.070589Z",
     "iopub.status.idle": "2023-12-01T17:23:13.831520Z",
     "shell.execute_reply": "2023-12-01T17:23:13.830883Z"
    },
    "executionInfo": {
     "elapsed": 409764,
     "status": "ok",
     "timestamp": 1701534201269,
     "user": {
      "displayName": "배현웅",
      "userId": "14643794117352663059"
     },
     "user_tz": -540
    },
    "id": "HgKusrbFnCE2",
    "outputId": "0fe49e82-4c3c-40dd-c367-7aeb2aed087c"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "visualize_latent_dim(trained_encoder_VAE, title='VAE',position='left')\n",
    "visualize_latent_dim(trained_encoder_AE, title='AE',position='right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SAChw292FYhk"
   },
   "source": [
    "## The generated image of the VAE depending on the latent vector dimension\n",
    "From now, the latent vector of the VAE is 2-dimensional. Let's change the dimension of latent vector with 5,10 and see the change of results. Here, the model is trained for 15 epochs since the larger dimension of latent vector needs more training. After training, you can see the generated image of the VAE depending on the latent vector dimension with the same noise, `eps`. It might be very marginal difference, but you can see the difference between the generated image of the VAE depending on the latent vector dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-12-01T17:32:11.340833Z",
     "iopub.status.busy": "2023-12-01T17:32:11.340615Z",
     "iopub.status.idle": "2023-12-01T17:45:42.605851Z",
     "shell.execute_reply": "2023-12-01T17:45:42.605296Z"
    },
    "executionInfo": {
     "elapsed": 1475559,
     "status": "ok",
     "timestamp": 1701533605985,
     "user": {
      "displayName": "배현웅",
      "userId": "14643794117352663059"
     },
     "user_tz": -540
    },
    "id": "SsW1BWyuFYhl",
    "outputId": "ab877d05-9829-489e-e675-cac5df783f67"
   },
   "outputs": [],
   "source": [
    "from HW4_1_YourAnswer import *\n",
    "VAEs = []\n",
    "config.epoch = 20\n",
    "for latent_dim in [5,10]:\n",
    "    # train VAE\n",
    "    config.latent_dim = latent_dim\n",
    "    train_loader = dataloader(train=True, batch_size=config.batch_size)\n",
    "    test_loader = dataloader(train=False, batch_size=config.batch_size)\n",
    "    encoder_high_dim = Encoder(hidden_dims = config.hidden_dims, latent_dim=config.latent_dim,model_name='VAE').to(device)\n",
    "    decoder_high_dim = Decoder(hidden_dims = config.hidden_dims, latent_dim=config.latent_dim,expand_dim=config.expand_dim).to(device)\n",
    "\n",
    "    trainer_high_dim = training_VAE(train_loader, test_loader,encoder_high_dim, decoder_high_dim, device, config, \\\n",
    "                           save_img=False,model_name='VAE',beta=1,img_show=False)\n",
    "    results_high_dim = trainer_high_dim.train()\n",
    "    trained_encoder_VAE_high_dim = results_high_dim['encoder']\n",
    "    trained_decoder_VAE_high_dim = results_high_dim['decoder']\n",
    "    VAEs.append((latent_dim,trained_encoder_VAE_high_dim,trained_decoder_VAE_high_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 310
    },
    "executionInfo": {
     "elapsed": 2055,
     "status": "ok",
     "timestamp": 1701533608021,
     "user": {
      "displayName": "배현웅",
      "userId": "14643794117352663059"
     },
     "user_tz": -540
    },
    "id": "WQgoa8QyFYhl",
    "outputId": "1a5ed3ab-c43a-416d-c4da-c08ffc07cc03"
   },
   "outputs": [],
   "source": [
    "from HW4_1_YourAnswer import dataloader\n",
    "test_loader = dataloader(train=False, batch_size=64)\n",
    "for image,label in test_loader:\n",
    "    image = image.to(device)\n",
    "    break\n",
    "plt.figure(figsize=(10, 10))\n",
    "eps = torch.randn(64, 20).to(device)\n",
    "for i in range(len(VAEs)):\n",
    "    latent_dim,trained_encoder_VAE,trained_decoder_VAE = VAEs[i]\n",
    "    _, _, rp = trained_encoder_VAE(image,eps[:,:latent_dim])\n",
    "    output = trained_decoder_VAE(rp)\n",
    "    output = torchvision.utils.make_grid(output.cpu().detach(), nrow=8)\n",
    "    output = output.numpy().transpose(1,2,0)\n",
    "    plt.subplot(1,3, i+1)\n",
    "    plt.title(f\"VAE latent dim : {latent_dim}\")\n",
    "    plt.imshow(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
