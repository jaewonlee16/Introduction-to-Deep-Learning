{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Linear classifier (numpy)\n",
    "\n",
    "In this exercise, we will learn to implement a Linear classifier using numpy modules. The goal here is to show the basic concepts of classification and to learn how to implement \n",
    "forward/backward propagation of operations used in Linear classifier. \n",
    "\n",
    "You will be asked to complete several functions/classes used below in `HW_YourAnswer.py`.\n",
    "\n",
    "- `softmax`\n",
    "- `cross_entropy_loss`\n",
    "- `linear_predict`\n",
    "- `linear_cost_func`\n",
    "- `batch_gradient_descent_func`\n",
    "- `stochastic_gradient_descent_func`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's check if you are properly using GPU\n",
    "- Ouput should be 'True'\n",
    "- If not, please follow the instructions in ETL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print('GPU available?:', use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's mount your drive directory to current notebook and change the system directory to your working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import sys\n",
    "sys.path.append('/content/drive/My Drive/your_working_dir')\n",
    "\n",
    "import os\n",
    "os.chdir('/content/drive/My Drive/your_working_dir')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from utils import *\n",
    "from HW_YourAnswer import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the CIFAR10 dataset.\n",
    "\n",
    "CIFAR10 dataset is composed of 50000 train data and 10000 test data but we will only use 10000, 1000 images each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10000 images for training data and 1000 images for test data\n",
    "# 'get_cifar10_data()' returns train/test data with its mean subtracted for training\n",
    "X_tr, Y_tr, X_te, Y_te, mean_img = get_CIFAR10_data()\n",
    "print ('Train data shape : %s,  Train labels shape : %s' % (X_tr.shape, Y_tr.shape))\n",
    "print ('Test data shape : %s,  Test labels shape : %s' % (X_te.shape, Y_te.shape))\n",
    "\n",
    "nb_classes = 10\n",
    "\n",
    "# one-hot encode labels from integers(0, 1, .., 9) to 10-dim one-hot vector\n",
    "train_targets = Y_tr.reshape(-1)\n",
    "Y_tr_onehot = np.eye(nb_classes)[train_targets]\n",
    "\n",
    "test_targets = Y_te.reshape(-1)\n",
    "Y_te_onehot = np.eye(nb_classes)[test_targets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize training images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of classes for cifar10\n",
    "class_names = ['airplane','automobile','bird','cat','deer',\n",
    "               'dog','frog','horse','ship','truck']\n",
    "\n",
    "# random data idx for visualization\n",
    "images_index = np.int32(np.round(np.random.rand(18,)*10000,0))\n",
    "\n",
    "fig, axes = plt.subplots(3, 6, figsize=(18, 6),\n",
    "                         subplot_kw={'xticks': [], 'yticks': []})\n",
    "\n",
    "fig.subplots_adjust(hspace=0.3, wspace=0.05)\n",
    "\n",
    "for ax, idx in zip(axes.flat, images_index):\n",
    "    # mean is added again to restore original image\n",
    "    img = (X_tr[idx,:3072].reshape(32, 32, 3) + mean_img.reshape(32, 32, 3))\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(class_names[Y_tr[idx]])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Softmax function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To do:\n",
    "- Implement `softmax` in `HW_YourAnswer.py` file <br> \n",
    "\n",
    "<center> \n",
    "\n",
    "$$ \n",
    "\\Large p(\\mathbf{Y}=k|\\mathbf{X}=x) =  \\Large \\frac{exp(\\mathbf{s_{k}})} {\\Sigma_{j=1}^{K} exp(s_{j})}\n",
    "$$\n",
    "(p.21 Leture 4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The output should be [2.06115362e-09 1.80485138e-35 9.99999998e-01]\n",
    "- Sum of the softmax output should be 0.9999999999999999\n",
    "- It should not be NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_x = np.array([[2060,2000,2080]])\n",
    "softmax_result1 = softmax(temp_x)\n",
    "print('Softmax result :\\n',softmax_result1)\n",
    "print ('\\nSum of the softmax :',np.sum(softmax_result1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The output should be <br>\n",
    "    [2.06115362e-09   1.80485138e-35   9.99999998e-01] <br>\n",
    "    [2.06106005e-09   4.53978686e-05   9.99954600e-01]]\n",
    " \n",
    "- Sum of the softmax output should be [1. 1.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_x = np.array([[2060,2000,2080],[1010,1020,1030]])\n",
    "softmax_result2 = softmax(temp_x)\n",
    "print('Softmax result :\\n',softmax_result2)\n",
    "print ('\\nSum of the softmax :',np.sum(softmax_result2,axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cross-Entropy Loss\n",
    "Here, we will consider cross-entropy loss between the true target value(e.g., 1) and prediced value after softmax. \n",
    "Note that Negative log-likelihood for the true class and cross entropy between true target  and prediction has a similar formula (Lecture 4 p.23)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To do:\n",
    "- Implement `cross_entropy_loss` in `HW_YourAnswer.py` file \n",
    "\n",
    "<center> \n",
    "\n",
    "$$ \n",
    "\\Large L(W) = \\frac{1}{N} \\Sigma_{i=1}^{N} L_{i}{(s_{i},y_{i})}\n",
    "$$\n",
    "    where \n",
    "\n",
    "$$\n",
    "L_{i}{(s_{i},y_{i})} = \\frac{exp(\\mathbf{s_{y_{i}}})} {\\Sigma_{j=1}^{K} exp(s_{j})}\n",
    "\\\\\n",
    " N : \\text{number of data}\n",
    "\\\\\n",
    "s_{i} : \\text{softmax score for } i^{th} \\text{ class}\n",
    "$$\n",
    "\n",
    "<br> NOTE: cross entropy loss is __averaged__ w.r.t the number of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The output should be 20.72326583694641\n",
    "- It should not be NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_score0 = np.array([[0.0, 0.0, 0.0]])\n",
    "temp_target0 = np.array([[0,1,0]])\n",
    "loss0 = cross_entropy_loss(temp_score0, temp_target0)\n",
    "print('Total Loss for temp_0 =', loss0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The output should be 1.2039728009926025\n",
    "- It should not be NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_score1 = np.array([[0.1, 0.3, 0.6]])\n",
    "temp_target1 = np.array([[0,1,0]])\n",
    "loss1 = cross_entropy_loss(temp_score1, temp_target1)\n",
    "print('Total Loss for temp_1 =', loss1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The output should be 0.7418746816378242\n",
    "- It should not be NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_score2 = np.array([[0.1, 0.3, 0.6],[0.2,0.4,0.4],[0.9,0.05,0.05]])\n",
    "temp_target2 = np.array([[0,1,0],[0,0,1],[1,0,0]])\n",
    "loss2 = cross_entropy_loss(temp_score2, temp_target2)\n",
    "print('Total Loss for temp_2 =', loss2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Linear classifier\n",
    "\n",
    "With several functions we've defined above, we will implement Linear classifier learned in Lecture 5 p.31-35. \n",
    "\n",
    "One thing to note is that the __column__ of weight matrix indicates weight for each class, which is different from notations used in the Lecture slides. \n",
    "$$\n",
    "W_{i}: i^{th} \\text{column of the matrix } = \\text{ classifier for class } i\n",
    "$$\n",
    "So, the score(before softmax) will be computed as \n",
    "$$\n",
    "XW+b\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('fig/Affine_Layer_pic.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To do:\n",
    "- Implement `linear_predict`, `linear_cost_func`, `batch_gradient_descent_func` in `HW_YourAnswer.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use only 20 data for example\n",
    "x_batch = X_tr[:20]\n",
    "y_batch = Y_tr_onehot[:20]\n",
    "\n",
    "print ('Train data shape : %s,  Train labels shape : %s' % (x_batch.shape, y_batch.shape))\n",
    "\n",
    "\n",
    "# Initialize weights & bias\n",
    "W = np.zeros((3072, 10))\n",
    "b = np.zeros((10))\n",
    "\n",
    "# Parameters for the gradient descent\n",
    "iterations = 100\n",
    "alpha = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- cost shoule be 2.302585082994045"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_cost = linear_cost_func(x_batch, y_batch, W, b)\n",
    "print('loss:', initial_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- final loss should be 0.018715975170939064"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_time = time.time()\n",
    "\n",
    "W_batch, _, J_his_batch, W_his_batch = batch_gradient_descent_func(x_batch, y_batch, W, b, alpha, iterations)\n",
    "print ('final loss : {} with {:.3f}s'.format(J_his_batch[-1], time.time()-s_time))\n",
    "\n",
    "# print(J_his_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we only use 20 data for practice, the loss curve drops dramatically in few iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotCostOpt(J_his_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To do:\n",
    "- Implement `stochastic_gradient_descent_func` in `HW_YourAnswer.py` file.\n",
    "- Due to randomness, the returned loss may be slightly different from the loss described in the script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize weights and biases\n",
    "W = np.zeros((3072, 10))\n",
    "b = np.zeros((10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Final loss should be 0.005614035128227841 (It doens't have to be exactly the same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 7\n",
    "mini_batch = 2\n",
    "\n",
    "W_SGD, _, J_his_SGD, W_his_SGD = stochastic_gradient_descent_func(x_batch, y_batch, W, b, alpha, iterations, mini_batch, random_seed)\n",
    "print ('final loss : {} with {:.3f}s'.format(J_his_SGD[-1], time.time()-s_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotCostOpt(J_his_SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the learned classifier\n",
    "\n",
    "Let's see the performance of our linear classifier trained with all the training data. Also, by rescaling the weight parameters, we can visuailize the learned weights for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now use all the data to train our linear classifier\n",
    "\n",
    "# 1. Training dataset \n",
    "train_X = X_tr\n",
    "train_Y = Y_tr_onehot\n",
    "\n",
    "# 2. Test dataset\n",
    "test_X = X_te\n",
    "test_Y = Y_te\n",
    "\n",
    "print ('Train data shape : %s,  Train labels shape : %s' % (train_X.shape, train_Y.shape))\n",
    "print ('Test data shape : %s,  Test labels shape : %s' % (test_X.shape, test_Y.shape))\n",
    "\n",
    "# 3. Training details\n",
    "W = np.zeros((3072, 10))\n",
    "b = np.zeros((10))\n",
    "alpha = 0.01\n",
    "iterations = 3000\n",
    "mini_batch = 10\n",
    "\n",
    "# 4. Train\n",
    "W_SGD, b_SGD, J_his_SGD, W_his_SGD = stochastic_gradient_descent_func(train_X, train_Y, W, b, alpha, iterations, mini_batch, random_seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using `linear_pred` which should be already implemented, let's evaluate on test data\n",
    "- Test acc should be about 0.383"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.argmax(linear_pred(test_X, W_SGD, b_SGD), axis=1)\n",
    "print('Test acc : ', np.sum(pred==test_Y)/len(pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the learned weight of each classes\n",
    "\n",
    "Since our model is not that accurate, these may not look nice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['airplane','automobile','bird','cat','deer',\n",
    "               'dog','frog','horse','ship','truck']\n",
    "\n",
    "\n",
    "w = W_SGD[:,:].reshape(32, 32, 3, 10) \n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "\n",
    "images_index = np.int32(np.round(np.random.rand(10,)*10000,0))\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(18, 6),\n",
    "                         subplot_kw={'xticks': [], 'yticks': []})\n",
    "\n",
    "fig.subplots_adjust(hspace=0.3, wspace=0.05)\n",
    "# print(w_min, w_max, visualize_theta)\n",
    "\n",
    "for idx, ax in enumerate(axes.flat):\n",
    "    wimg = 255.0 * (w[:, :, :, idx].squeeze() - w_min) / (w_max - w_min)\n",
    "    img = (wimg.reshape(32, 32, 3))\n",
    "    ax.imshow(img.astype('uint8'))\n",
    "    ax.set_title(class_names[idx])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Neural Network Modules (Numpy)\n",
    "\n",
    "In this exercise, we will learn to implement a Twolayer Neural Network using numpy modules. The goal here is to learn how to implement forward/backward propagation of each operations used in Neural Network and how to modularize each operations. \n",
    "\n",
    "You will be asked to complete several functions/classes used below in `HW_YourAnswer.py`.\n",
    "\n",
    "- `OutputLayer`\n",
    "- `ReLU`\n",
    "- `Sigmoid`\n",
    "- `Affine`\n",
    "- `TwoLayerNet`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Output Layer\n",
    "\n",
    "Now let's consider output layer that forwards cross entropy loss with given score and target in class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To do:\n",
    "- Implement `OutputLayer` in `HW_YourAnswer.py` file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('fig/Output_Layer.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputlayer = OutputLayer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Forward output should be 13.095060867144417\n",
    "- Backward output should be [ 0.90887517 -0.99999795  0.09112277]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_x1 = np.array([[3, -10, 0.7]])\n",
    "temp_t1 = np.array([[0,1,0]])\n",
    "output_forward1 = outputlayer.forward(temp_x1, temp_t1)\n",
    "output_backward1 = outputlayer.backward()\n",
    "print('Forward propagation of output layer :', output_forward1)\n",
    "print('Backward propagation of output layer :', output_backward1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Forward output should be 7.075548386844261\n",
    "- Backward output should be [ 3.02958391e-01, -3.33332649e-01,  3.03742579e-02],\n",
    "       [-3.32509126e-01,  3.32509088e-01,  3.74189683e-08],\n",
    "       [ 7.26173786e-04,  2.92959414e-01, -2.93685588e-01]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_x2 = np.array([[3, -10, 0.7],[9,15,-1],[-5,1,-1]])\n",
    "temp_t2 = np.array([[0,1,0],[1,0,0],[0,0,1]])\n",
    "output_forward2 = outputlayer.forward(temp_x2, temp_t2)\n",
    "output_backward2 = outputlayer.backward()\n",
    "print('Forward propagation of output layer :', output_forward2)\n",
    "print('\\nBackward propagation of output layer :', output_backward2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To do:\n",
    "- Implement `ReLU` in `HW_YourAnswer.py` file <br><br> \n",
    "\n",
    "<center>\n",
    "\n",
    "$$ \n",
    "\\Large ReLU(x) =  max(0,x) \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relu = ReLU()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Forward propagation should be [3.  0.  0.7]\n",
    "- Backward propagation should be [-10   0   0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_x1 = np.array([[3, -10, 0.7]])\n",
    "temp_x2 = np.array([[-10,1,0]])\n",
    "relu_forward1 = relu.forward(temp_x1)\n",
    "relu_backward1 = relu.backward(temp_x2)\n",
    "print('Forward propagation of ReLU :', relu_forward1)\n",
    "print('Backward propagation of ReLU :', relu_backward1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Forward propagation should be \n",
    "            [ 3. ,  0. ,  0.7],\n",
    "            [ 9. , 15. ,  0. ],\n",
    "            [ 0. ,  1. ,  0. ]\n",
    "<br>\n",
    "- Backward propagation should be\n",
    "            [  3,   0, -10],\n",
    "            [  5,  -4,   0],\n",
    "            [  0,  -5,   0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_x3 = np.array([[3, -10, 0.7],[9,15,-1],[-5,1,-1]])\n",
    "temp_x4 = np.array([[3,5,-10],[5,-4,2],[-3,-5,3]])\n",
    "relu_forward2 = relu.forward(temp_x3)\n",
    "relu_backward2 = relu.backward(temp_x4)\n",
    "print('Forward propagation of ReLU :', relu_forward2)\n",
    "print('\\nBackward propagation of ReLU :', relu_backward2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To do:\n",
    "- Implement `Sigmoid` in `HW_YourAnswer.py` file <br><br> \n",
    "\n",
    "<center> \n",
    "\n",
    "$$\n",
    "\\Large  \\sigma(x) =  \\frac{1}{1+exp^{-x}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid = Sigmoid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Forward propagation output should be [9.52574127e-01 4.53978687e-05 6.68187772e-01]\n",
    "- Backward propagation output should be [ 0.13552998 -0.00045396  0.15519901]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_x1 = np.array([[3, -10, 0.7]])\n",
    "sigmoid_forward1 = sigmoid.forward(temp_x1)\n",
    "sigmoid_backward1 = sigmoid.backward(temp_x1)\n",
    "print('Forward propagation of sigmoid :',sigmoid_forward1)\n",
    "print('Backward propagation of sigmoid :',sigmoid_backward1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Forward propagation output should be <br>\n",
    "   [9.52574127e-01 4.53978687e-05 6.68187772e-01], <br>\n",
    "   [9.99876605e-01 9.99999694e-01 2.68941421e-01], <br>\n",
    "   [6.69285092e-03 7.31058579e-01 2.68941421e-01]\n",
    "\n",
    "- Backward propagation output should be  <br>\n",
    " [ 1.35529979e-01 -4.53958077e-04  1.55199011e-01], <br>\n",
    " [ 1.11041415e-03  4.58853200e-06 -1.96611933e-01], <br>\n",
    " [-3.32402834e-02  1.96611933e-01 -1.96611933e-01]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_x2 = np.array([[3, -10, 0.7],[9,15,-1],[-5,1,-1]])\n",
    "sigmoid_forward2 = sigmoid.forward(temp_x2)\n",
    "sigmoid_backward2 = sigmoid.backward(temp_x2)\n",
    "print('Forward propagation of sigmoid :',sigmoid_forward2)\n",
    "print('\\nBackward propagation of sigmoid :',sigmoid_backward2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Affine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To do:\n",
    "- Implement `Affine` in `HW_YourAnswer.py` file <br><br> \n",
    "- __Note__ : bias is added seperately.\n",
    "\n",
    "<center>   \n",
    "\n",
    "$$\n",
    "Affine(W,b) =  XW + b\n",
    "$$\n",
    "\n",
    "- Note that the matrix multiplication are implemented in XW not WX which are different from the Lecture slides. Also this time, bias is considered seperately (not included in the weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Forward propagation output should be <br>\n",
    "[ 0.51 -0.39  0.84] <br>\n",
    " [-0.07 -0.02  0.02]\n",
    " \n",
    "- Backward propagation output should be <br>\n",
    "[-0.61  0.28] <br>\n",
    " [-0.25 -0.21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_W = np.array([[0.2, -0.3, 0.6], [-0.9, 0.1, -0.4]])\n",
    "temp_b = np.array([[0.2, -0.3, 0.6]])\n",
    "temp_x = np.array([[0.2, -0.3], [-0.9, 0.1]])\n",
    "temp_t = np.array([[0.1, 0.5, -0.8], [0.4, 0.7, -0.2]])\n",
    "\n",
    "affine = Affine(temp_W, temp_b)\n",
    "affine_forward1 = affine.forward(temp_x)\n",
    "affine_backward1 = affine.backward(temp_t)\n",
    "print('Forward propagation of Affine :\\n', affine_forward1)\n",
    "print('\\nBackward propagation of Affine :\\n', affine_backward1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- dW of affine should be <br>\n",
    "        [-0.34, -0.53,  0.02],\n",
    "        [ 0.01, -0.08,  0.22]\n",
    "<br>\n",
    "- db of affine should be\n",
    "        [ 0.5,  1.2, -1. ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dw = affine.dW\n",
    "db = affine.db\n",
    "print('Gradient of the weights :\\n', dw)\n",
    "print('\\nGradient of the biases :',db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. TwoLayerNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To do:\n",
    "- Implement `TwoLayerNet` in `HW_YourAnswer.py` file <br><br> \n",
    "- Note : Our TwoLayerNN will be using L2-regularization to prevent overfitting.\n",
    "- Use OrderedDict to make a model (https://pymotw.com/2/collections/ordereddict.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical gradient vs Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Running time of grad_backprop should be much faster than the time of grad_numerical "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = TwoLayerNet(input_size=3072, hidden_size=10, output_size=10, regularization = 100)\n",
    "\n",
    "x_batch = X_tr[:20]\n",
    "y_batch = Y_tr_onehot[:20]\n",
    "\n",
    "start_time = time.time() \n",
    "grad_backprop = network.gradient(x_batch, y_batch)\n",
    "print(\"[grad_backprop] running time(sec) : \" +str(time.time() - start_time))\n",
    "\n",
    "start_time = time.time() \n",
    "grad_numerical = network.numerical_gradient(x_batch, y_batch)\n",
    "print(\"[grad_numerical] running time(sec) : \"+str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the difference is trivial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in grad_numerical.keys():\n",
    "    diff = np.average( np.abs(grad_backprop[key] - grad_numerical[key]) )\n",
    "    print(key + \":\" + str(diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train our TwoLayerNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_size_=3072\n",
    "hidden_size_=1024\n",
    "output_size_=10\n",
    "regularization_ = 0.0001\n",
    "\n",
    "network = TwoLayerNet(input_size=input_size_, hidden_size=hidden_size_, output_size=output_size_, regularization = regularization_)\n",
    "\n",
    "# NOTE: you may change iters_num while debugging your code\n",
    "# but before submission, please submit the final model trained with 2000 iteratinos.\n",
    "iters_num = 2000\n",
    "train_size = X_tr.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list_two = []\n",
    "train_acc_list_two = []\n",
    "test_acc_list_two = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(iters_num):\n",
    "\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = X_tr[batch_mask]\n",
    "    y_batch = Y_tr_onehot[batch_mask]\n",
    "\n",
    "    _ = network.gradient(x_batch, y_batch) \n",
    "    \n",
    "    network.update_params(learning_rate)\n",
    "\n",
    "    loss = network.loss(x_batch, y_batch)\n",
    "    train_loss_list_two.append(loss)    \n",
    "\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(X_tr, Y_tr_onehot)\n",
    "        test_acc = network.accuracy(X_te, Y_te_onehot)\n",
    "        train_acc_list_two.append(train_acc)\n",
    "        test_acc_list_two.append(test_acc)\n",
    "\n",
    "        print(\"Epoch : \",i / iter_per_epoch + 1, \"Training acc : \", round(train_acc,2), \"Test acc : \", round(test_acc,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_plot(train_acc_list_two,test_acc_list_two)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "albef",
   "language": "python",
   "name": "albef"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
