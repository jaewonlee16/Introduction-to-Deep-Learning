{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW3: Sentiment Analysis via Deep Neural Networks\n",
    "\n",
    "This homework consists of two parts:\n",
    "\n",
    "1. You will implement recurrent neural network (RNN), long short-term memory (LSTM), and multi-head attention from scratch, and compare the results with `torch.nn` modules.\n",
    "2. You will build a text encoder based on the implemented modules and train it on the sentiment analysis task of the movie review dataset(IMDb). Then, evaluate the training results.\n",
    "\n",
    "You may be asked to complete several functions and classes in `HW_YourAnswer_modules.py` and `HW_YourAnswer_encoders.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import sys\n",
    "sys.path.append('/content/drive/My Drive/Colab Notebooks/Intro_dl/hw3')\n",
    "\n",
    "import os\n",
    "os.chdir('/content/drive/My Drive/Colab Notebooks/Intro_dl/hw3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from utils import normal\n",
    "\n",
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: DNN Models for Sequential Data\n",
    "\n",
    "In the first exercise, you will attempt to implement deep neural network models designed for processing sequential data.\n",
    "\n",
    "## 1.1. Recurrent Neural Network(RNN)\n",
    "\n",
    "First, let's implement a multi-layer RNN, and then compare it with `torch.nn.RNN`. To do this, we will proceed in two steps. \n",
    "\n",
    "1. Develop the `OneLayerRNN` module to represent a single-layer RNN.\n",
    "2. Utilize the `OneLayerRNN` module implemented in step 1 to construct the `MultiLayerRNN` module, enabling the computation of a multi-layer RNN.\n",
    "\n",
    "In this exercise, it is ***strongly*** recommended to refer to [1] for further details.\n",
    "\n",
    "[1] https://pytorch.org/docs/stable/generated/torch.nn.RNN.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1. One Layer RNN\n",
    "\n",
    "One-layer RNN aims to compute the hidden state for each token when given an input sequence. The formula for this is as follows:\n",
    "\n",
    "$$\n",
    "h_t = \\text{activation}(W_{ih}x_t+ b_{ih} + W_{hh}h_{t-1} + b_{hh})\n",
    "$$\n",
    "\n",
    "\n",
    "The operation of linear projection and adding bias here can be achieved by using the `nn.Linear` module, which was employed in the previous assignment.\n",
    "\n",
    "\n",
    "### To Do:\n",
    "- Implement `OneLayerRNN` class in `HW_YourAnswer_modules.py` file.\n",
    "\n",
    "In order to compute a multi-layer RNN in the future, it is necessary to return not only the final hidden state required for classification but also all hidden states corresponding to the entire input sequence. For detailed instructions, refer to the documentation in the respective file.\n",
    "\n",
    "The following code directly compares the computational results of your module and the PyTorch built-in module `nn.RNN`. L2 norm is used as the loss function to check for backward passes. More details can be found in `module_check.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: OneLayerRNN\n",
      "batch_size: 64\n",
      "input_size: 128\n",
      "hidden_size: 128\n",
      "n_seq: 20\n",
      "\n",
      "Forward MSE for output layer\t: 0.00000e+00\n",
      "Forward MSE the last h\t\t: 0.00000e+00\n",
      "Backward MSE\t\t\t: 3.63090e-15\n"
     ]
    }
   ],
   "source": [
    "from HW_YourAnswer_modules import OneLayerRNN\n",
    "from module_check import Info, check_one_layer_rnn\n",
    "\n",
    "# Setting hyperparameters\n",
    "info = Info(device = DEVICE)\n",
    "\n",
    "input_size = info.input_size\n",
    "hidden_size = info.hidden_size\n",
    "\n",
    "\n",
    "# Implemented one-layer RNN(yours)\n",
    "one_layer_rnn = OneLayerRNN(input_size = input_size, hidden_size = hidden_size).to(DEVICE)\n",
    "# RNN from torch.nn\n",
    "target_rnn = nn.RNN(input_size = input_size, hidden_size = hidden_size, batch_first = True).to(DEVICE)\n",
    "\n",
    "# Function for comparison\n",
    "check_one_layer_rnn(one_layer_rnn, target_rnn, info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference: `nn.Sequential`\n",
    "\n",
    "`torch.nn.Sequential` is a container module that allows you to construct neural networks in a sequential manner. It is a convenient way to sequentially organize a series of layers in a neural network. You can pass a list of layers to torch.nn.Sequential, and it will execute them in the order they are passed. Each layer's output serves as the input to the next layer. This sequential arrangement simplifies the code, making it more readable and easier to manage.\n",
    "\n",
    "\n",
    "The following example provides a demonstration to aid in understanding the module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaiveNet forward result\t\t: 0.704\n",
      "SequentialNet forward result\t: 0.704\n"
     ]
    }
   ],
   "source": [
    "# MLP w/o nn.Sequential\n",
    "class NaiveNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear0 = nn.Linear(2, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear1 = nn.Linear(2, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.linear0(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear1(out)\n",
    "        return out\n",
    "\n",
    "# MLP w/ nn.Sequential\n",
    "class SequentialNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        linear = [nn.Linear(2, 2), nn.Linear(2, 1)]\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear = nn.Sequential(*linear)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear[0](x)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear[1](out)\n",
    "        return out\n",
    "    \n",
    "naive_net = NaiveNet()\n",
    "sequential_net = SequentialNet()\n",
    "\n",
    "# parameter matching\n",
    "for p1, p2 in zip(naive_net.parameters(), sequential_net.parameters()):\n",
    "    p2.data = p1.data\n",
    "\n",
    "x = normal((2,))\n",
    "print(f\"NaiveNet forward result\\t\\t: {naive_net(x).item():.3f}\")\n",
    "print(f\"SequentialNet forward result\\t: {sequential_net(x).item():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2. Multi-Layer RNN\n",
    "\n",
    "Next, let's implement a module to perform multi-layer RNN operations based on the `OneLayerRNN` we implemented earlier. A multi-layer RNN takes as input the output hidden states of the previous layer at each layer.\n",
    "\n",
    "$$\n",
    "h_t^{\\ell} = \\text{activation}(W_{ih}^{\\ell}h_t^{\\ell-1} + b_{ih}^{\\ell} + W_{hh}^{\\ell}h_{t-1}^{\\ell} + b_{hh}^{\\ell})\n",
    "$$\n",
    "\n",
    "You may use `nn.Sequential` to define any number of layers you want. If implemented simply via a list, it will not be stored in the module's buffer and may cause problems with device allocation. For other details, see the instruction in the file.\n",
    "\n",
    "### To Do:\n",
    "- Implement `MultiLayerRNN` class in `HW_YourAnswer_modules.py` file.\n",
    "\n",
    "The following code will compare the forward and backward of `nn.RNN` with the module you implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: MultiLayerRNN\n",
      "batch_size: 64\n",
      "input_size: 128\n",
      "hidden_size: 128\n",
      "num_layers: 3\n",
      "n_seq: 20\n",
      "\n",
      "Forward MSE for output layer\t: 0.00000e+00\n",
      "Forward MSE the last h\t\t: 0.00000e+00\n",
      "Backward MSE\t\t\t: 3.68443e-13\n"
     ]
    }
   ],
   "source": [
    "from HW_YourAnswer_modules import MultiLayerRNN\n",
    "from module_check import Info, check_multi_layer_rnn\n",
    "\n",
    "# Setting hyperparameters\n",
    "info = Info(device = DEVICE)\n",
    "\n",
    "input_size = info.input_size\n",
    "hidden_size = info.hidden_size\n",
    "num_layers = info.num_layers\n",
    "\n",
    "# Implemented multi-layer RNN(yours)\n",
    "multi_layer_rnn = MultiLayerRNN(input_size = input_size, hidden_size = hidden_size, num_layers = num_layers).to(DEVICE)\n",
    "# RNN from torch.nn\n",
    "target_rnn = nn.RNN(input_size = input_size, hidden_size = hidden_size, num_layers = num_layers, batch_first = True).to(DEVICE)\n",
    "\n",
    "# Function for comparison\n",
    "check_multi_layer_rnn(multi_layer_rnn, target_rnn, info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2: Long Short-Term Memory(LSTM)\n",
    "\n",
    "Second, let's implement a multi-layer LSTM and compare it to `torch.nn.LSTM`.\n",
    "1. Develop the `OneLayerLSTM` module to represent a single-layer LSTM\n",
    "2. Utilize the `OneLayerLSTM` module implemented in step 1 to construct the `MultiLayerLSTM` module, enabling the computation of a multi-layer LSTM.\n",
    "\n",
    "In this exercise, it is ***strongly*** recommended to refer to [2] for further details.\n",
    "\n",
    "[2] https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1. One Layer LSTM\n",
    "\n",
    "LSTMs are similar to vanilla RNNs in that they output hidden states, but there are differences in the computation process.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "i_t & = \\sigma\\left( W_{ii}x_t + b_{ii} + W_{hi}h_{t-1} + b_{hi} \\right) \\\\\n",
    "f_t & = \\sigma\\left( W_{if}x_t + b_{if} + W_{hf}h_{t-1} + b_{hf} \\right) \\\\\n",
    "g_t & = \\text{tanh}\\left( W_{ig}x_t + b_{ig} + W_{hg}h_{t-1} + b_{hg} \\right) \\\\\n",
    "o_t & = \\sigma\\left( W_{io}x_t + b_{io} + W_{ho}h_{t-1} + b_{ho} \\right) \\\\\n",
    "c_t & = f_t \\odot c_{t-1} + i_t \\odot g_t \\\\\n",
    "h_t & = o_t \\odot \\text{tanh}(c_t)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "If we use the `nn.Linear` module for each gate, we would need a total of 8 modules. However, ***considering that only two types of inputs are used, $x_t$ and $h_{t-1}$, we can see that the linear projection can be computed in parallel by quadrupling the output dimension of the linear projections and then dividing the outputs by four***. Use this fact to implement a one-layer LSTM.\n",
    "\n",
    "### To Do:\n",
    "- Implement `OneLayerLSTM` class in `HW_YourAnswer_modules.py` file.\n",
    "\n",
    "Similar to the implementation of RNNs, LSTMs require hidden state throughout the sequence to implement later multi-layer versions. See the instruction for additional details. The following code will compare the forward and backward of `nn.LSTM` with the module you implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: OneLayerLSTM\n",
      "batch_size: 64\n",
      "input_size: 128\n",
      "hidden_size: 128\n",
      "n_seq: 20\n",
      "\n",
      "Forward MSE for output layer\t: 3.51448e-16\n",
      "Forward MSE the last h\t\t: 3.18767e-16\n",
      "Forward MSE the last c\t\t: 9.65108e-16\n",
      "Backward MSE\t\t\t: 1.52876e-14\n"
     ]
    }
   ],
   "source": [
    "from HW_YourAnswer_modules import OneLayerLSTM\n",
    "from module_check import Info, check_one_layer_lstm\n",
    "\n",
    "# Setting hyperparameters\n",
    "info = Info(device = DEVICE)\n",
    "\n",
    "input_size = info.input_size\n",
    "hidden_size = info.hidden_size\n",
    "\n",
    "# Implemented one-layer LSTM(yours)\n",
    "one_layer_lstm = OneLayerLSTM(input_size = input_size, hidden_size = hidden_size).to(DEVICE)\n",
    "# LSTM from torch.nn\n",
    "target_lstm = nn.LSTM(input_size = input_size, hidden_size = hidden_size, batch_first = True).to(DEVICE)\n",
    "\n",
    "# Function for comparision\n",
    "check_one_layer_lstm(one_layer_lstm, target_lstm, info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2. Multi-Layer LSTM\n",
    "\n",
    "Let's implement a module to perform multi-layer LSTM operations based on the `OneLayerLSTM` you implemented earlier. Similar to RNNs, a multi-layer LSTM takes as input the output hidden states of the previous layer at each layer.\n",
    "\n",
    "### To Do:\n",
    "- Implement `MultiLayerLSTM` class in `HW_YourAnswer_modules.py` file.\n",
    "\n",
    "The following code will compare the forward and backward of `nn.LSTM` with the module you implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: MultiLayerLSTM\n",
      "batch_size: 64\n",
      "input_size: 128\n",
      "hidden_size: 128\n",
      "num_layers: 3\n",
      "n_seq: 20\n",
      "\n",
      "Forward MSE for output layer\t: 8.82961e-17\n",
      "Forward MSE the last h\t\t: 1.30734e-16\n",
      "Forward MSE the last c\t\t: 4.09939e-16\n",
      "Backward MSE\t\t\t: 4.76482e-14\n"
     ]
    }
   ],
   "source": [
    "from HW_YourAnswer_modules import MultiLayerLSTM\n",
    "from module_check import Info, check_multi_layer_lstm\n",
    "\n",
    "# Dataset\n",
    "info = Info(device = DEVICE)\n",
    "\n",
    "input_size = info.input_size\n",
    "hidden_size = info.hidden_size\n",
    "num_layers = info.num_layers\n",
    "\n",
    "# Implemented multi-layer LSTM(yours)\n",
    "multi_layer_lstm = MultiLayerLSTM(input_size = input_size, hidden_size = hidden_size, num_layers = num_layers).to(DEVICE)\n",
    "# LSTM from torch.nn\n",
    "target_lstm = nn.LSTM(input_size = input_size, hidden_size = hidden_size, num_layers = num_layers, batch_first = True).to(DEVICE)\n",
    "\n",
    "# Function for comparision\n",
    "check_multi_layer_lstm(multi_layer_lstm, target_lstm, info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3: Multi-head Attention\n",
    "\n",
    "Next, let's implement a module to perform multi-head attention and compare it to `torch.nn.MultiheadAttention`. To do this, proceed with the following 3 steps\n",
    "1. Define a module that linearly projects the query, key, and value vectors before performing the attention operation.\n",
    "2. Define a function to split the linearly projected query, key, and value into multiple heads\n",
    "3. Define a function to perform scaled dot-product attention based on the results of 1 and 2, and define a forward function.\n",
    "\n",
    "In this exercise, it is ***strongly*** recommended to refer to [3] and [4] for further detail.\n",
    "\n",
    "[3] Vaswani, A., et al, \"Attention is All you Need,\" in *Advances in Neural Information Processing Systems*, 2017.<br>\n",
    "[4] https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1. Linear Projections\n",
    "\n",
    "Scaled dot-product attention is computed in vectorized form by the following process.\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V\n",
    "$$\n",
    "\n",
    "where $d_k$ is the dimension of the linearly projectionquery, key, and value vectors. Using the above expression, we can describe multi-head attention as follows.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{Multihead}(Q, K, V) & = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W^O \\\\\n",
    "\\text{where head}_i & = \\text{Attention}(QW_i^Q, KW_i^K, WV_i^V)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Based on the above expression, we need (3 * $\\texttt{num\\_heads}$ + 1) linear projections. However, since the projected query and key value required for each head all have the same input, we can simplify the calculation by using only one projection for each and then dividing the result by $\\texttt{num\\_heads}$. Thus, a total of four linear projections are required for multi-head attention. Before we start implementing the attention mechanism, let's define the projection modules.\n",
    "\n",
    "### To Do:\n",
    "- Implement the `__init__` function of the `MultiheadAttention` class inside the `HW_YourAnswer_modules.py` file.\n",
    "\n",
    "You can see what submodules the `MultiheadAttention` module you implemented has in the code block below.\n",
    "\n",
    "Note: The `nn.MultiheadAttention` module which we target uses a trainable bias initialized with a zero vector for both input and output projection. Therefore, there is no need to use the ($\\texttt{bias = False}$) argument when using the `nn.Linear` module.\n",
    "\n",
    "If implemented correctly, the output from running the code block below should look like this.\n",
    "\n",
    "```\n",
    "MultiheadAttention(\n",
    "  (q_proj): Linear(in_features=6, out_features=6, bias=True)\n",
    "  (k_proj): Linear(in_features=3, out_features=6, bias=True)\n",
    "  (v_proj): Linear(in_features=3, out_features=6, bias=True)\n",
    "  (out_proj): Linear(in_features=6, out_features=6, bias=True)\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiheadAttention(\n",
      "  (q_proj): Linear(in_features=6, out_features=6, bias=True)\n",
      "  (k_proj): Linear(in_features=3, out_features=6, bias=True)\n",
      "  (v_proj): Linear(in_features=3, out_features=6, bias=True)\n",
      "  (out_proj): Linear(in_features=6, out_features=6, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from HW_YourAnswer_modules import MultiheadAttention\n",
    "from module_check import Info\n",
    "\n",
    "# Setting hyperparameters\n",
    "info = Info(\n",
    "    device = DEVICE,\n",
    "    batch_size = 1,\n",
    "    n_seq = 2,\n",
    "    embed_dim = 6,\n",
    "    num_heads = 3,\n",
    "    kdim = 3,\n",
    "    vdim = 3\n",
    ")\n",
    "\n",
    "embed_dim = info.embed_dim\n",
    "num_heads = info.num_heads\n",
    "kdim = info.kdim\n",
    "vdim = info.vdim\n",
    "\n",
    "# Initialized multi-head attention module(yours)\n",
    "my_attention = MultiheadAttention(\n",
    "    embed_dim = embed_dim,\n",
    "    num_heads = num_heads,\n",
    "    kdim = kdim,\n",
    "    vdim = vdim\n",
    "    ).to(DEVICE)\n",
    "\n",
    "print(my_attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2. Head Splitting\n",
    "\n",
    "In order to perform Attention in a multi-head manner, we need to divide the input query, key, and value by the number of heads. To put this in terms of tensor size, a tensor with the shape $\\texttt{(batch\\_size, n\\_seq, embed\\_dim)}$ should be replaced with $\\texttt{(batch\\_size, num\\_heads, n\\_seq, embed\\_dim // num\\_heads)}$. The position of each dimension is to simplify the attention process later on.\n",
    "\n",
    "### To Do:\n",
    "- Implement the `split_heads` function of the `MultiheadAttention` class inside the `HW_YourAnswer_modules.py` file.\n",
    "\n",
    "Suppose the arguments required for execution are defined as follows:\n",
    "```\n",
    "batch_size = 1\n",
    "n_seq = 2\n",
    "embed_dim = 6\n",
    "num_heads = 3\n",
    "```\n",
    "If implemented correctly, the output from running the code block below should look like this\n",
    "\n",
    "```\n",
    "Original Tensor\n",
    "shape: torch.Size([1, 2, 6])\n",
    "[[[ 0  1  2  3  4  5]\n",
    "  [ 6  7  8  9 10 11]]]\n",
    "\n",
    "Splitted Tensor\n",
    "shape: torch.Size([1, 3, 2, 2])\n",
    "[[[[ 0  1]\n",
    "   [ 6  7]]\n",
    "\n",
    "  [[ 2  3]\n",
    "   [ 8  9]]\n",
    "\n",
    "  [[ 4  5]\n",
    "   [10 11]]]]\n",
    "```\n",
    "\n",
    "In the result above, you can see that the embedding vector, corresponding to the third dimension of the original tensor, is divided into thirds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tensor\n",
      "shape: torch.Size([1, 2, 6])\n",
      "[[[ 0  1  2  3  4  5]\n",
      "  [ 6  7  8  9 10 11]]]\n",
      "\n",
      "Splitted Tensor\n",
      "shape: torch.Size([1, 3, 2, 2])\n",
      "[[[[ 0  1]\n",
      "   [ 6  7]]\n",
      "\n",
      "  [[ 2  3]\n",
      "   [ 8  9]]\n",
      "\n",
      "  [[ 4  5]\n",
      "   [10 11]]]]\n"
     ]
    }
   ],
   "source": [
    "from utils import tensor_print\n",
    "\n",
    "# You can change the hyperparameters to check your results; default is (1, 2, 6, 3)\n",
    "batch_size = 1\n",
    "n_seq = 2\n",
    "embed_dim = 6\n",
    "num_heads = 3\n",
    "\n",
    "my_attention = MultiheadAttention(\n",
    "    embed_dim = embed_dim,\n",
    "    num_heads = num_heads,\n",
    "    kdim = kdim,\n",
    "    vdim = vdim\n",
    "    ).to(DEVICE)\n",
    "\n",
    "x = torch.arange(batch_size * n_seq * embed_dim).reshape((batch_size, n_seq, embed_dim)).to(DEVICE)\n",
    "\n",
    "print(\"Original Tensor\")\n",
    "tensor_print(x)\n",
    "print()\n",
    "\n",
    "print(\"Splitted Tensor\")\n",
    "tensor_print(my_attention.split_heads(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference: `torch.Tensor.masked_fill`\n",
    "\n",
    "`Tensor.masked_fill` is a method that allows you to fill elements of a tensor with a specified value based on a provided mask. This method is useful for selectively updating values in a tensor according to a condition specified by the mask.\n",
    "\n",
    "The following code block is an example of how `Tensor.masked_fill` can be utilized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tensor:\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])\n",
      "\n",
      "Mask:\n",
      "tensor([[ True, False,  True],\n",
      "        [False,  True, False],\n",
      "        [ True, False,  True]])\n",
      "\n",
      "Result after masked_fill:\n",
      "tensor([[-1,  2, -1],\n",
      "        [ 4, -1,  6],\n",
      "        [-1,  8, -1]])\n"
     ]
    }
   ],
   "source": [
    "# Create a sample tensor\n",
    "tensor = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "\n",
    "# Define a mask with the same shape as the tensor\n",
    "mask = torch.tensor([[True, False, True], [False, True, False], [True, False, True]])\n",
    "\n",
    "# Use masked_fill to replace values where the mask is True with a specified value, e.g., -1\n",
    "result = tensor.masked_fill(mask = mask, value = -1)\n",
    "\n",
    "print(\"Original Tensor:\")\n",
    "print(tensor)\n",
    "print(\"\\nMask:\")\n",
    "print(mask)\n",
    "print(\"\\nResult after masked_fill:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.3. Scaled Dot-Product Attention\n",
    "\n",
    "Now let's complete the module by implementing scaled dot-product attention. The overall procedures can be calculated using the formula provided below.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{Attention}(Q, K, V) & = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V \\\\\n",
    "\\text{Multihead}(Q, K, V) & = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W^O \\\\\n",
    "\\text{where head}_i & = \\text{Attention}(QW_i^Q, KW_i^K, WV_i^V)\n",
    "\\end{align*}\n",
    "$$ \n",
    "\n",
    "### To Do:\n",
    "- Implement the `scaled_dot_product_attention` function of the `MultiheadAttention` class inside the `HW_YourAnswer_modules.py` file.\n",
    "- Implement the `forward` function of the `MultiheadAttention` class inside the `HW_YourAnswer_modules.py` file.\n",
    "\n",
    "The important thing is to put masking on the padding token before computing the distribution via attention score. ***Masking of the attention score can be accomplished by substituting a very small number (e.g. -1e9) in the position corresponding to the padding token***. For this purpose, it is convenient to use the `Tensor.masked_fill` method. For more details about masking, see the references and instructions in the file.\n",
    "\n",
    "The following code will compare the forward and backward of `nn.MultiheadAttention` with the module you implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: MultiheadAttention\n",
      "batch_size: 64\n",
      "n_seq: 20\n",
      "n_key: 10\n",
      "embed_dim: 128\n",
      "kdim: 64\n",
      "vdim: 64\n",
      "num_heads: 8\n",
      "\n",
      "Forward MSE for the output: 0.00000e+00\n",
      "Forward MSE for the attention weights: 0.00000e+00\n",
      "Backward MSE: 1.90970e-14\n"
     ]
    }
   ],
   "source": [
    "from module_check import Info, check_multi_head_attention\n",
    "\n",
    "# Setting hyperparameters\n",
    "info = Info(device = DEVICE)\n",
    "\n",
    "embed_dim = info.embed_dim\n",
    "num_heads = info.num_heads\n",
    "kdim = info.kdim\n",
    "vdim = info.vdim\n",
    "\n",
    "# Implemented multi-head attention(yours)\n",
    "my_attention = MultiheadAttention(embed_dim = embed_dim,\n",
    "                                  num_heads = num_heads,\n",
    "                                  kdim = kdim,\n",
    "                                  vdim = vdim).to(DEVICE)\n",
    "# Multi-head attention from torch.nn\n",
    "target_attention = nn.MultiheadAttention(embed_dim = embed_dim,\n",
    "                                         num_heads = num_heads,\n",
    "                                         kdim = kdim,\n",
    "                                         vdim = vdim,\n",
    "                                         batch_first = True).to(DEVICE)\n",
    "\n",
    "# Function for comparison\n",
    "check_multi_head_attention(my_attention, target_attention, info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Sentiment Analysis of IMDb Movie Reviews\n",
    "\n",
    "In the second exercise, you will use the deep neural networks you implemented earlier to analyze the sentiment of text data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Data Processing and nn.Embedding\n",
    "\n",
    "Let's start by fetching the data and then processing it. For the textual data, we use the Large Movie Review Dataset [5]. The Large Movie Review Dataset is a dataset consisting of 50,000 movie reviews from IMDb, a movie information site in the United States, each labeled with a positive or negative label to train a machine on the task of determining whether the sentiment in a sentence is positive or negative.\n",
    "\n",
    "\n",
    "\n",
    "[5] Maas, A., et al, \"Learning Word Vectors for Sentiment Analysis,\" in *Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies*, 2011, pp. 142–150."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1. Data Loading\n",
    "\n",
    "Let's download the data first. It may take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-11-29 18:26:49--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
      "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
      "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 84125825 (80M) [application/x-gzip]\n",
      "Saving to: ‘aclImdb_v1.tar.gz’\n",
      "\n",
      "aclImdb_v1.tar.gz   100%[===================>]  80.23M  1.73MB/s    in 77s     \n",
      "\n",
      "2023-11-29 18:28:06 (1.04 MB/s) - ‘aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download data\n",
    "!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "!tar -zxf ./aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see samples of the downloaded data in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Positive review example\n",
      "####################################################################################################\n",
      "While rehearing Carmen of Bizet, the middle-aged choreographer Antonio (Antonio Gades) brings the se\n",
      "xy Carmen (Laura del Sol) to perform the lead role. Antonio falls in love for Carmen, who is an inde\n",
      "pendent and seductive woman incapable to accept a possessive love. When Carmen has an affair with an\n",
      "other dancer, Antonio is consumed by his jealousy like D. José in the original opera, entwining fict\n",
      "ion with reality.\n",
      "\n",
      "\"Carmen\" is another great movie of Carlos Saura's trilogy dedicated to the Flamenco dance. The drama\n",
      "tic love story is developed with the lives of the artists entwined with the characters they are rehe\n",
      "arsing, and many times is not absolutely clear whether what is happening is reality (with the dancer\n",
      "s) or fiction (of the play). Paco de Lucia is another attraction of this original version of the fam\n",
      "ous Bizet's opera, which is based on the novel of Prosper Mérimée. My vote is seven.\n",
      "\n",
      "Title (Brazil): \"Carmen\"\n",
      "####################################################################################################\n",
      "\n",
      "2. Negative review example\n",
      "####################################################################################################\n",
      "This documentary is a reenactment of the last few years of Betty Page's(Paige Richards) career. The \n",
      "Tennessee tease was the most recognizable pin-up queen in history. Her most memorable work came in t\n",
      "he 1950's and was fetish photos, bondage and cat-fight \"girly flicks\". Irving Klaw(Dukey Flyswatter)\n",
      "at his Movie Star News instructed Betty on what to do in front of the camera. There was no nudity in\n",
      " the famous photos or \"stag films\", but nonetheless, Klaw was charged with distributing obscene mate\n",
      "rials and was ordered to destroy them to avoid prosecution. It is no surprise that Betty had a cult \n",
      "following at the height of her career. The girl-next-door with jet black hair, blue eyes and an hour\n",
      " glass figure dressed in fetish gear or not would mesmerize for decades. After all, it has been said\n",
      " that she was photographed more than Marilyn Monroe and second only to the most photographed image i\n",
      "n the world, Elvis Presley. Betty Page would disappear and devote her last years to religion. This m\n",
      "ovie actually could have been a lot better; but good enough to hold interest.\n",
      "\n",
      "Miss Richards is stunning in her own right. Bra, panties, garter belt and hose do not hurt her image\n",
      " in the least. Also in the cast: Jaimie Henkin, Jana Strain, Emily Marilyn and Julie Simone. Be advi\n",
      "sed this movie can change your heart rate.\n",
      "####################################################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from imdb import Imdb\n",
    "\n",
    "from train import train\n",
    "from utils import review_print, model_print, predict\n",
    "\n",
    "# Dataset\n",
    "imdb = Imdb()\n",
    "vocabs = imdb.vocabs\n",
    "num_tokens = imdb.num_tokens\n",
    "pos_text = imdb.pos_text\n",
    "neg_text = imdb.neg_text\n",
    "\n",
    "# Reviews\n",
    "rand_idx = np.random.randint(num_tokens)\n",
    "print(\"1. Positive review example\")\n",
    "review_print(pos_text[rand_idx])\n",
    "print(\"2. Negative review example\")\n",
    "review_print(neg_text[rand_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2. Text Processing and nn.Embedding\n",
    "\n",
    "Before text data can be used to train a neural network, several steps of preprocessing are required.\n",
    "\n",
    "1. Create a dictionary that assigns a token to each word.\n",
    "2. Make a list of each word in the given text.\n",
    "3. Assign a token to each word.\n",
    "4. Assign an embedding to each token.\n",
    "\n",
    "The IMDb dataset provides a ranking of the frequency of each word, so the dictionary is based on that file. If we process the data in this way, $\\texttt{batch\\_size}$ text data will be transformed into $\\texttt{(batch\\_size, n\\_seq, embeding\\_dim)}$.\n",
    "\n",
    "### Reference: `torch.nn.Embedding`\n",
    "`torch.nn.Embedding` is a module that allows you to create an embedding layer for representing discrete tokens as continuous vectors. This layer is often used in natural language processing tasks where words or tokens need to be converted into dense vectors for input to a neural network.\n",
    "\n",
    "The embedding layer is initialized with the vocabulary size (num_embeddings) and the dimensionality of the embedding vectors (embedding_dim). During the forward pass, this layer takes integer-encoded input indices and returns their corresponding dense vector representations.\n",
    "\n",
    "The following example demonstrates the initialization and usage of the nn.Embedding layer, with IMDb dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Raw text\t\t: This is a testing sentence for HW3!\n",
      "2. After preprocessing\t: ['this', 'is', 'a', 'testing', 'sentence', 'for', 'hw3', '!'] (length 8)\n",
      "3. After tokenization\t: [2, 13, 9, 6, 7619, 4232, 17, 1, 31, 3] (length 10)\n",
      "4. After Embedding\t:\n",
      "[[ 0.3477485   0.0954706   0.72874254]\n",
      " [ 0.35365325  1.0004146   0.6053533 ]\n",
      " [-0.4440546   0.05737263 -0.38004556]\n",
      " [-0.00223519 -0.08045699 -0.88316053]\n",
      " [-0.12144136 -0.69452184  0.61660904]\n",
      " [-0.59110355  1.6351695  -0.54890543]\n",
      " [-1.4244665  -1.1882875   0.03491776]\n",
      " [ 0.1069      0.7165252  -1.5920839 ]\n",
      " [-0.06048542 -0.09256128 -0.44705513]\n",
      " [ 0.98898363 -0.5523288   0.1347568 ]] (shape (10, 3))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from imdb import preprocessing\n",
    "\n",
    "embedding_dim = 3\n",
    "embedding = nn.Embedding(num_tokens, embedding_dim)\n",
    "\n",
    "# Text preprecessing and the tokenizer\n",
    "tokenizer = imdb.tokenizer\n",
    "# You can input your own test textual data if interested\n",
    "test = \"This is a testing sentence for HW3!\"\n",
    "\n",
    "print(\"1. Raw text\\t\\t:\", test)\n",
    "print(\"2. After preprocessing\\t:\", preprocessing(test), f\"(length {len(preprocessing(test))})\")\n",
    "print(\"3. After tokenization\\t:\", tokenizer(test), f\"(length {len(tokenizer(test))})\")\n",
    "embedded = embedding(torch.tensor(tokenizer(test), dtype = torch.int32))\n",
    "print(\"4. After Embedding\\t:\")\n",
    "print(embedded.detach().numpy(), f\"(shape {tuple(embedded.shape)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2: RNN Encoder\n",
    "\n",
    "Now let's create an encoder using the RNN modules. The most naive encoder structure is to view the last hidden state as a latent vector containing information about the sentence. For this assignment, ***we will build one that applies a binary linear classifier to the last hidden state***. See the instructions in the file for details.\n",
    "\n",
    "### To Do:\n",
    "- Implement `RNNEncoder` class in `HW_YourAnswer_encoders.py` file.\n",
    "- Use `torch.nn.RNN` module to implement the RNN layers\n",
    "\n",
    "Using your own implementation of `MultiLayerRNN` would be a good exercise. However, unlike the `torch.nn` module, your `MultiLayerRNN` does not have any acceleration techniques and will take a very long time to train. Therefore, ***we recommends using the*** `nn.RNN` ***module to implement*** `RNNEncoder`.\n",
    "\n",
    "If implemented correctly, `MultiLayerRNN` and `nn.RNN` perform the same operations and produce the same output. However, one caveat is that when using `nn.RNN`, you must force $\\texttt{batch\\_first = True}$ as an argument. This argument means that the first dimension of the input data corresponds to the batch index.\n",
    "\n",
    "The next block of code is intended to verify that the implementation is correct by overfitting the model using only 8 data points. Your test accuracy should be 1.00, as the model was trained only on a small dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test loss:  0.0000| Test accuracy:  1.00\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from HW_YourAnswer_encoders import RNNEncoder\n",
    "\n",
    "# Small dataloader from IMDb dataset; it consists of 20 text data\n",
    "loaders = imdb.make_small_loaders()\n",
    "\n",
    "# Hyperparameters\n",
    "embedding_dim, hidden_dim = 256, 256\n",
    "\n",
    "# Implemented RNN encoder(yours)\n",
    "rnn = RNNEncoder(num_tokens = num_tokens,\n",
    "                 embedding_dim = embedding_dim,\n",
    "                 hidden_dim = hidden_dim, \n",
    "                 num_layers = 3).to(DEVICE)\n",
    "\n",
    "# Function for training\n",
    "train(rnn, loaders, epochs = 20, lr = 1e-3, small_loader = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've completed your sanity check, it's time to try training on full data. If implemented correctly, you should be able to achieve a test accuracy of around 0.7.\n",
    "\n",
    "### To Do:\n",
    "- Train your `RNNEncoder` model and report the test accuracy.\n",
    "\n",
    "You can also see the prediction results of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full dataloader\n",
    "loaders = imdb.make_loaders(batch_size = 128)\n",
    "\n",
    "train(rnn, loaders, epochs = 10, lr = 5e-5)\n",
    "\n",
    "model_print(rnn, pos_text, neg_text, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try inserting your own text to see the prediction results if you are interested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I love this movie\"\n",
    "label = 1\n",
    "predict(rnn, text, label, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3: LSTM Encoder\n",
    "\n",
    "Next, let's create an encoder using an LSTM. Again, using your `MultiLayerLSTM` will require a very long training time, so we recommend using the `nn.LSTM` module. The same as in RNN, you need to force the $\\texttt{batch\\_first}$ argument to $\\texttt{True}$.\n",
    "\n",
    "### To Do:\n",
    "- Implement `LSTMEncoder` class in `HW_YourAnswer_encoders.py` file.\n",
    "\n",
    "The next block of code is intended to verify that the implementation is correct by overfitting the model using only 8 data points. Similarly, the test accuracy should be 1.00."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HW_YourAnswer_encoders import LSTMEncoder\n",
    "\n",
    "# Small dataloader from IMDb dataset; it consists of 20 text data\n",
    "loaders = imdb.make_small_loaders()\n",
    "\n",
    "embedding_dim, hidden_dim = 256, 256\n",
    "\n",
    "# Implemented LSTM encoder(yours)\n",
    "lstm = LSTMEncoder(num_tokens = num_tokens,\n",
    "                 embedding_dim = embedding_dim,\n",
    "                 hidden_dim = hidden_dim, \n",
    "                 num_layers = 3)\n",
    "\n",
    "# Function for training\n",
    "train(lstm, loaders, epochs = 20, lr = 5e-3, small_loader = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try training your `LSTMEncoder` model on full data. If implemented correctly, you should be able to achieve a test accuracy of around 0.8.\n",
    "\n",
    "### To Do:\n",
    "- Train your `LSTMEncoder` model and report the test accuracy.\n",
    "\n",
    "You can also see the prediction results of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders = imdb.make_loaders(batch_size = 128)\n",
    "train(lstm, loaders, epochs = 10, lr = 1e-4)\n",
    "\n",
    "model_print(lstm, pos_text, neg_text, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try inserting your own text to see the prediction results if you are interested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I love this movie\"\n",
    "label = 0\n",
    "predict(lstm, text, label, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4: Transformer Encoder\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?export=download&id=1HhPbR5jzc1DG_8z5yba2v71q95ntEKbP\" width=\"30%\"></center>\n",
    "\n",
    "Finally, let's build an encoder using multi-head attention. In [3], several techniques are added to the encoder in addition to multi-head attention to improve performance. These include\n",
    "\n",
    "- Positional Encoding\n",
    "- Layer Normalization (we omit this)\n",
    "- Feed Forward Network\n",
    "\n",
    "The original Transformer architecture includes both encoder and decoder layers, but for the purposes of our challenge, we will only implement the encoder.\n",
    "\n",
    "Again, it is ***strongly*** recommended to refer to [3] for further detail.\n",
    "\n",
    "[3] Vaswani, A., et al, \"Attention is All you Need,\" in *Advances in Neural Information Processing Systems*, 2017."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.1. Feed Forward Network\n",
    "\n",
    "First, let's implement a feed forward network. A feed forward network consists of two fully connected layers.\n",
    "\n",
    "### To Do:\n",
    "- Implement the `FeedForward` class in `HW_YourAnswer_encoders.py`.\n",
    "\n",
    "See the instruction in the file for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2. Positional Encoding\n",
    "\n",
    "In the Transformer architecture, positional encoding is used to provide the model with information about the positions of elements in the input sequence. Unlike RNN, Transformers don't inherently capture sequential order because they process inputs in parallel. Positional encoding helps the model understand the sequential order of the input by injecting positional information into the input embeddings.\n",
    "\n",
    "The positional encoding is added to the input embeddings before feeding them into the Transformer model. It is usually a sine and cosine function of different frequencies, which allows the model to differentiate between different positions in the sequence.\n",
    "\n",
    "Here is the formula for positional encoding:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "PE_{(pos, 2i)} & = \\sin (pos / 10000^\\frac{2i}{d_\\text{model}}) \\\\\n",
    "PE_{(pos, 2i+1)} & = \\cos (pos / 10000^\\frac{2i}{d_\\text{model}})\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "### To Do:\n",
    "- Define `pos_enc` inside `TransformerEncoder.__init__` in the `HW_YourAnswer_encoders.py` file.\n",
    "\n",
    "One implementation detail is to ***add the singleton dimension beforehand to broadcast for the batch dimension***. See the instruction in the file for details.\n",
    "\n",
    "If implemented correctly, you should get the following figure when you run the code block below.\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?export=download&id=1CtkoEv7RmxlampMX7ukY-P9sAK9q_lkN\" width=\"100%\"></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HW_YourAnswer_encoders import TransformerEncoder\n",
    "from utils import pos_enc_print\n",
    "\n",
    "embedding_dim, hidden_dim = 256, 256\n",
    "\n",
    "# Implemented transformer(yours)\n",
    "transformer = TransformerEncoder(num_tokens = num_tokens,\n",
    "                                 embedding_dim = embedding_dim,\n",
    "                                 hidden_dim = hidden_dim,\n",
    "                                 num_layers = 1).to(\"cuda\")\n",
    "\n",
    "# Positional encoding\n",
    "pos_enc = transformer.pos_enc[0, :101, :].cpu().numpy()\n",
    "\n",
    "pos_enc_print(pos_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.3. Forward\n",
    "\n",
    "Now we are all set to implement the `TransformerEncoder`.\n",
    "\n",
    "### To Do:\n",
    "- Implement `EncoderBlock` class in `HW_YourAnswer_encoders.py` file.\n",
    "- Implement `TransformerEncoder` class in `HW_YourAnswer_encoders.py` file.\n",
    "\n",
    "The `EncoderBlock` contains one `MultiheadAttention` layer and one `FeedForward` layer. Note that ***you need to do a residual sum after each layer***.\n",
    "\n",
    "The `TransformerEncoder` is not much different from `RNNEncoder` or `LSTMEncoder` except that it uses `EncoderBlock` as a module to handle recurrence. However, there is a major difference in the position of the padding token \\<PAD\\> when configuring the dataloader. In the case of RNN and LSTM, the \\<EOS\\> token that signals the end of the sentence is used as a latent vector, and the \\<PAD\\> token corresponding to padding is concatenated at the beginning of the sentence to fix the position of the \\<EOS\\> token. This is because RNNs and LSTMs can convey sequential information well without positional encoding. Conversely, the Transformer cannot adopt this padding strategy since all tokens are simultaneously processed. Therefore, when implementing `TransformerEncoder`, you should use a \\<SOS\\> token, which indicates the start of the sentence, as a latent vector. This assumes that the padding is concatenated after the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HW_YourAnswer_encoders import TransformerEncoder\n",
    "\n",
    "# Small dataloader from the IMDb dataset\n",
    "loaders = imdb.make_small_loaders(pad_first = False)\n",
    "\n",
    "embedding_dim, hidden_dim = 256, 256\n",
    "\n",
    "# Implemented Transformer encoder(yours)\n",
    "transformer = TransformerEncoder(num_tokens = num_tokens,\n",
    "                                 embedding_dim = embedding_dim,\n",
    "                                 hidden_dim = hidden_dim,\n",
    "                                 num_layers = 1).to(\"cuda\")\n",
    "\n",
    "train(transformer, loaders, epochs = 50, lr = 1e-4, use_mask = True, small_loader = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try training your `TransformerEncoder` model on full data. If implemented correctly, you should be able to achieve a test accuracy of around 0.8.\n",
    "\n",
    "### To Do:\n",
    "- Train your `TransformerEncoder` model and report the test accuracy.\n",
    "\n",
    "You can also see the prediction results of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders = imdb.make_loaders(batch_size = 8, pad_first = False)\n",
    "embedding_dim, hidden_dim = 256, 256\n",
    "\n",
    "transformer = TransformerEncoder(num_tokens = num_tokens,\n",
    "                                 embedding_dim = embedding_dim,\n",
    "                                 hidden_dim = hidden_dim,\n",
    "                                 num_layers = 1)\n",
    "\n",
    "train(transformer, loaders, epochs = 5, lr = 1e-4, use_mask = True)\n",
    "\n",
    "model_print(transformer, pos_text, neg_text, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try inserting your own text to see the prediction results if you are interested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I love this movie\"\n",
    "label = 1\n",
    "predict(transformer, text, label, tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
